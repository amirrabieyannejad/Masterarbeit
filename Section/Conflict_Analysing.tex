\section{Analysing Conflict}\label{conflict}
In software development, especially in agile methods, conflict analysis is an important task to ensure the coherence and functionality of the system to be developed. A conflict is defined as a inconsistency that arises when two or more requirements, often encapsulated as USs, contradict each other. This section will introduce and define conflict analysis, focusing on the concept of \textit{content inconsistency} between USs.

The main objective of this analysis is to rationalise the software development workflow by semantically identifying conflicts between the USs within the backlog of a project.

A conflict of requirements arises when two or more USs show contradictions or inconsistencies. This can manifest itself in various forms, e.g. in the manipulation of the same resource by several USs at the same time, in overlapping functions or in conflicting conditions.

\begin{example}
	Considering following USs:\\\\
	user\_story\_13: "\#G03\# As a Staff member, I want to Apply a Hold, so that I can prevent progression through the workflow or other actions in the system until the issue is resolved."\\\\
	user\_story\_14: "\#G03\# As a Staff member, I want to Remove a Hold, so that I can allow progression through the workflow or other actions in the system now that the issue has been resolved."\\\\
	In this example, user\_story\_14 deletes a resource that is used by user\_story\_13. This means that user\_story\_13 cannot be applied at all if user\_story\_14 is executed first, which leads to a conflict between these USs.
\end{example}

In section \ref{conflict_requirement}, we present the requirements and functional needs that serve as input for the design phase in order to fulfil the requirements. In section \ref{conflict_desing} we explain the design decisions of the workflow shown in figure \ref{fig:conflict_operational_flow} and explain how the architecture is structured. 

\subsection{Requirements}\label{conflict_requirement}
In order to accomplish the analysis of conflicts in USs we try to address following functional requirements:
\begin{itemize}
	
	\item As a user, I want to perform semantic analysis on user stories within a specified project backlog, so that I can identify and address conflicts effectively.
	
	\item As a user, I want a report on the US-pairs that are conflicting in the main parts so I can change them if needed.
	
	\item As a user, I want to apply a filter to the conflict report to only show US-pairs that have the same resource (as entity) with different verbs (as action), so that the verbs are semantically contradictory (e.g. one US deletes a resource that another US is using, or deleting or one US creates a resource that another US prohibits).
	
	%\item As a user, I want to mark found redundancy clauses as Triggers with a hash symbol (\#) and show those that have a redundancy in \enquote{Persona} (as a noun) and \enquote{Action} (as a verb) entries, so that I can better see if the persona in is also recognised as a redundancy.
	
	%\item As a user, I want to mark the containers (as Contains) with a hash symbol (\#) and display found resources as conflict element (as noun), so that I can better see whether the contained entity is also recognised as a conflict.
	
	\item As a user, I would like to have a conflict report that shows founded US texts in US-pairs and adds a hash symbol (\#) at the beginning and end of conflicting verbs and a noun (as a resource) as a marker, so that I can better recognise the verbs and noun that conflict in US-pairs.
	
	\item As a user, I want to see how many conflict US-pairs have been created in the main parts of the USs within a backlog, so that I can summarise conflict US-pairs founded on this basis for further statistical purposes.
	
	\item As a user, I want a table at the top of the conflict report that lists the US-pairs in conflict, so that I can quickly see all the US-pairs that have been founded.
	
\end{itemize}
To judge the operation of a system, we define following non-functional requirements:
\begin{itemize}
	\item Testability: The system should support automated test procedures to ensure that semantic analysis and conflict detection work correctly. It should include comprehensive test cases covering different scenarios, including edge cases, to verify the accuracy and reliability of conflict detection.
	
	\item Documentation: The system should include detailed documentation covering all aspects of functionality and setup. User manuals, API documentation and troubleshooting guides should be provided. 
	
	\item Performance: The system should perform the conflict analysis within a reasonable time frame, even with large project backlogs. It should be optimised so that it can process large volumes of data without any significant loss of performance.
	
	\item Scalability: The system should be scalable to handle an increasing the number of USs and larger project backlogs.
		
\end{itemize}

\subsection{Design}\label{conflict_desing}
This section describes the operational flow and architectural considerations that underpin the framework.
\subsubsection*{Design Overview}
To address the requirements specified in Section \ref{conflict_requirement}, our system used the backlogs labelled with Doccano tool\footnote{https://github.com/ace-design/nlp-stories} generated by Mosser et al. as the primary input\cite{arulmohan2023extracting}.

To conduct conflict analysis, a one-time preparatory phase is needed to categorize the verbs in USs of backlog into four categories. Once this is done, conflict analysis can be applied to the USs using these categories and specific criteria.

Once the conflict analysis has been applied, a comprehensive report is created that contains information regarding conflict pairs in both a textual and a tabular form.

Finally, a conflict assessment phase is initiated for further statistical purposes.

Figure \ref{fig:conflict_operational_flow} illustrates how each step in this sequence is interconnected, with the output of one step feeding directly into the next. This diagram effectively demonstrates the toolchain and process workflow, highlighting how each step transforms artefacts and contributes to the overall objective of conflict detection.
\begin{figure}[h]
	\centering 
	\includegraphics[scale=0.65]{conflict_operational_flow}
	\caption{Step-by-step visualisation of the tool chain and its inputs and outputs}\label{fig:conflict_operational_flow}
\end{figure}
\paragraph{One-Time-Phase: Creating Database for annotated Verbs}In this phase, each verb should be classified using the VerbNet\footnote{\href{https://verbs.colorado.edu/verbnet}{https://verbs.colorado.edu/verbnet}} classification. Finally, each verb class annotated into four categories, namely \textit{Create, Delete, Preserve, or Forbid} (from now on) called \textit{action-annotation}. 

To validate the action annotations, a personal judgement is made for each verb by three evaluators, so that each person reviews the action annotations for each verb and comments their own action annotation. We then collect all the personal judgements and combine the action annotations for each verb.
%\paragraph{Translating the JSON-Format of the Primary Input}\label{conflict_workflow_preparing_json_format}
%Since the annotated USs in the original JSON files did not split entries such as "Entity", "Action", "Text", "Targets" and "Contains", it is not clear which element belongs to which part of the USs (main or benefit part), which leads to possible ambiguities.

%Accordingly, we use a class called \textit{USPartExtractor} that separates entries based on their occurrence in both the main and benefit parts of the USs. It also specifies an identifier to assign a unique identifier to each US, which is stored in a JSON object called \textit{"US\_Nr"}. 

%These additions improve the system's ability to distinguish and process individual USs within the analytical pipeline.
\paragraph{Conflict Analysis and Extraction of text reports} Creating a text report aims to find conflicts between USs and highlight important information, such as identifying potentially conflict pairs, the conflict reason, the resource (as noun) affected by actions (as verbs) causing the conflict, the texts of the main parts with the affected elements marked with \# and a tabulation of the potentially conflicting pairs.

\paragraph{Evaluating the reports} Once we have created the reports, we can now assess the correctness of the US-pairs reported as conflicts, i.e. whether the reported US-pairs really cause a conflict.

\subsubsection*{Software Architecture}\label{conflict_architectur}
In this section, we present the basic structures of our workflow and the discipline of creating such structures. Each structure comprises software elements, relations among them, and properties of both.
\begin{itemize}
	\item Annotated USs with Doccano Tool\footnote{https://github.com/doccano/doccano}: Mosser et al. used publicly available requirements from Dalpiaz et al.\cite{Dalpiaz2018} consisting of 19 product backlogs and 1,458 USs. The dataset is a raw archive of 19 text files, each containing one US per line. 
	
	As there were no public expert-based annotations, Mosser et al. manually annotated the dataset using the Doccano tool for \textit{Named Entity Recognition}. Labels included persona, action, entity, benefit part and relations such as triggers, targets, and contains based on their domain meta-model.
	
	As artefact we receive a graph-based model with JSON format, which represents the refined and annotated dataset for the recognition of \emph{entities}, \emph{actions}, \emph{personas} and \emph{benefits} of USs \cite{mosser2022modelling}.
	
	\item Eclipse as IDE\footnote{https://eclipseide.org/}: Eclipse is an integrated development environment (IDE) used in computer programming. It contains a base work workspace and an extensible plug-in system for customizing the environment.
	
	\item VerbNet as Verb Lexicon Resource\footnote{https://verbs.colorado.edu/verbnet/}: VN is the largest on-line network of English verbs that links their syntactic and semantic patterns. It is a hierarchical, domain-independent, broad-coverage verb lexicon with mappings to other lexical resource, such as WordNet\footnote{https://wordnet.princeton.edu/}, PropBank \footnote{https://propbank.github.io/}, and FrameNet \footnote{http://framenet.icsi.berkeley.edu/}. 
	
	VerbNet is organized into verb classes extending Levin (1993) classes through refinement and addition of subclasses to achieve syntactic and semantic coherence among members of a class. Each verb class in VN is completely described by thematic roles, selectional preferences of the arguments, and frames consisting of a syntactic description and a semantic representation with subevent structure patterned on the Dynamic Event Model of Pustejovsky and Moszkowicz and Pustejovsky\cite{kipper2006extending}.
	
	\item USPartExtractor Class: This class is part of the \textit{org.backlogconflict.code .preparation} package and is a key component of the software architecture designed for transforming primary input datasets in the specific JSON format. It separates elements based on their occurrence in both the main and benefit parts of the USs.
	
	This process simplifies subsequent tasks for editing, analysing and conflict resolution by providing a clear structure for the USs and their components. The separation of main and benefit parts and the assignment of unique identifiers improves the manageability and traceability of USs within the system.
	
	\item Action Annotation Reference Database: This database is essential for identifying conflicts between USs, especially conflicts arising from actions over common entities ( as resources). To achieve this, we categorise verbs into four different groups namely \textit{Preserve, Delete, Create, and Prohibit}.
	
	The main purpose of the action Annotation reference database is to facilitate the translation of actions (in the form of verbs) found in USs into corresponding action-annotations. This process involves several important steps:
	\begin{enumerate}
		\item Collection of Actions: We collect all actions (represented as verbs) from existing datasets and compile them into a CSV file. This file serves as comprehensive reference database.
		
		\item Contextual Translation: Each verb in the CSV file is translated into the corresponding action-annotations related to its VerbNet class. 
		
		\item Personal Judgement: To validate the action annotations, three evaluators individually assess each verb. Each evaluator reviews the annotations and provides their own comments. We then gather these individual assessments and combine them to finalize the action annotation reference database for each verb.
	\end{enumerate}
	
	\item VerbFinder Class: The \texttt{VerbFinder} class is an essential component within the \textit{org.backlogconflict.code.preparation} package, designed to interface with the action annotation database and facilitate the process of mapping verbs to their corresponding action annotations. 
	
	\item ActionsAnnotationsCreator class : This class is a component within the \textit{org.henshin. backlogconflict.code.preparation} package. Its primary function is to improve the extraction of Parts of US process by incorporating action annotations into the JSON dataset. 
	
	The class adds entries that consist of a set of triples: "action", "entity", and "action-annotations". These action-annotations are sourced from the reference database, which ensures that conflict detection and resolution are consistent and accurate. The matching process is done using the VerbFinder class. 
	
	\item ReportMaker Class: This class developed within the \textit{org.backlogconflict .code.report} packages, its primary function is to identify conflicting US-pairs based on specific criteria and generate comprehensive reports on these conflicts. It performs the following key tasks:%This class provides detailed information on the nature of the conflicts, making it an invaluable tool for maintaining system coherence and resolving inconsistencies. 
	\begin{enumerate}
		\item Identification of Conflict US-pairs: The class analysis USs to identify pairs that conflict based on predefined criteria. These criteria including conflicting actions, or inconsistencies in the USs.
		
		\item Detailed Conflict Reporting: Once conflicts are identified, the class generates detailed reports. These reports contain essential information such as the affected entity(as resource), potential conflicted actions, conflict reason, and the texts of main parts of the USs with marked elements with hash symbol (\#).
		
		\item Tabular Summary: In addition to detailed conflict descriptions, the class also produces a tabular summary of all identified conflict pairs in a backlog, providing a quick overview of the conflict US-pairs.
	\end{enumerate}

	
\end{itemize}
Figure \ref{fig:conflict_technical_implementation} shows the architectural composition, highlighting the integral components and their user interface and artefacts.
\begin{figure}[h]
	\centering 
	\includegraphics[scale=0.6]{conflict_technical_implementation}
	\caption{Design phases}\label{fig:conflict_technical_implementation}
\end{figure}
Regarding conflict, some definitions are clarified:
\begin{definition}[\textbf{Customized User Story}]
	In order to apply conflict analysis to the backlog, a customized user story is defined, which consists solely of the main part that collectively describes what the user wants and the consequences of this need for the resources.
\begin{itemize}
	\item The \textit{main part} is essential as it clearly and concisely summarizes the persona, the intended functionality, and the resources required to perform the action. This part usually follows the format: \textit{"As a [persona], I want [actions over entities]."}
	
	In this customized US, the intended functionality, which describe the action that the persona wants to perform or the function they need, will be translated into four annotations: "create," "delete", "preserve", or "forbid". These annotations serve to standardize the actions for conflict analysis:
	
	\begin{itemize}
		\item \textbf{Create:} This action-annotation describes the introduction or addition of a new entity within the system. For example, the action "apply" in US \textit{"As a staff member, I want to apply for a hold."} is annotated with "create" action.
		
		\item \textbf{Delete:} This action-annotation indicates the removal or elimination of an entity from the system. For example, the action "remove" in US \textit{"As a staff member, I want to remove a hold."} is annotated with "delete" action.
		
		\item \textbf{Preserve:} This action-annotation involves safeguarding, or using an existing entity without alterations. For example, the action "browse" in US \textit{"As a researcher, I want to browse through files in a collection."} is annotated with "preserve" action.
		
		\item \textbf{Forbid:} This action-annotation specifies prohibiting certain actions on an entity. For example, the action "restrict access" in US \textit{"As a collection curator, I want to restrict access to my collection or items to duke IP addresses."} is annotated with "forbid" action.
	\end{itemize}
	
	Specifying the resources required to perform the action helps with planning and resource allocation, ensuring that the development team is aware of the tools, technologies, and time required. This includes identifying all entities involved in the actions described and their relationships.
	
	In other words, with respect to the action, we translate it into the aforementioned action-annotations for conflict analysis. This translation standardizes the actions, making it easier to identify and resolve conflicts between USs.
	
	It is worth noting that in this form of US, the benefit part is not considered as part of the structure of USs. The focus is solely on the actions and resources, simplifying the US to its core components necessary for conflict analysis.

\end{itemize}
\end{definition}
\begin{definition}[\textbf{Conflict}]
	Conflict refers to situations where two USs try to:	 
	\begin{itemize}
		\item delete a resource which another US are using
		\item delete a resource which another US also wants to delete
		\item create a resource which another US prohibits
	\end{itemize}
	$Notation$. Lowercase identifiers refer to single elements, and uppercase identifiers denote sets. 
	\\A user story is a 1-tuple $us = \langle m\rangle $ where:
	\begin{itemize}
		\item A main $m$ is define a 6-tuple: \\\\$m = \langle p,A,E,Tr,Ta,Co\rangle $ \\\\where:
		
	\begin{itemize}
		\item $p$ is the persona.
		
		\item $A = \{ a_1,a_2,...\} $ is a set of actions.
		
		\item $E = \{e_1,e_2,...\}$ is a set of entities.
		
		\item $Tr = \{(p_1,a_1),(p_2,a_2),...\}$ is a set of trigger references, each begin a pair of persona and action.
		
		\item $Ta = \{(a_1,e_1,R_1),(a_2,e_2,R_2),...\}$ is a set of target references, each begin a triple of action, entity, and action-annotations $R$.
		
		\item $Co = \{ (e_{c1},e_{c2},R_{c1}),(e_{c*},e_{c*},R_{c2}),... \}$ is a set of contain references, each begin a triple of two entities and action-annotations R.
		
		\item $R = \{preserve, create, delete, forbid\}$ are the annotations applied to actions.
	\end{itemize}
	
	\end{itemize}
	To denote that a syntactic operator, we add the subscript
	“syn”; for instance, $=_{syn}$ is syntactic equivalence which introduced by Lucassen et al. \cite{lucassen2016improving}.\\ Consider two USs:\\\\ $us_1 = \langle m_1\rangle $ where $m_1 = \langle p_1,a_1,e_1,tr_1,ta_1,co_1 \rangle$ \\\\$us_2 = \langle m_2\rangle$ where $m_2 = \langle p_2,a_2,e_2,tr_2,ta_2,co_2 \rangle$ and $co_2 = (e_{c1},e_{c2},R_{c1})$\\\\
	$us_1$ causes a conflict if:
	\begin{enumerate}
		\item The entity $e_1$ is an exact redundant of entity $e_2$, formally:\\ $isRedundant(e_1,e_2) \leftrightarrow e_1 =_{syn} e_2$ and one of the following conditions holds:\\
		\begin{enumerate}
			\item $ta_1 = (e_1,a_1,"preserve")$ and $ta_2 = (e_2,a_2,"delete")$
			
			\item $ta_1 = (e_1,a_1,"create")$ and $ta_2 = (e_2,a_2,"forbid")$
			
			\item $ta_1 = (e_1,a_1,"delete")$ and $ta_2 = (e_2,a_2,"delete")$
		\end{enumerate}
		
		\item The entity $e_1$ is an exact redundant of  $e_{c1}$, formally:\\ $isRedundant(e_1,e_{c1}) \leftrightarrow e_1 =_{syn} e_{c1}$ and one of the following conditions holds:\\
		\begin{enumerate}
			\item $ta_1 = (e_1,a_1,"preserve")$ and $co_2 = (e_{c1},e_{c2},"delete")$
			
			\item $ta_1 = (e_1,a_1,"create")$ and $co_2 = (e_{c1},e_{c2},"forbid")$
			
			\item $ta_1 = (e_1,a_1,"delete")$ and $co_2 = (e_{c1},e_{c2},"delete")$
		\end{enumerate}
	\end{enumerate}
	 
	To comprehensively assess conflicts, it is important to consider not only the textual content but also the functional relevance of each action over entities within the USs. By categorizing actions into four groups, conflict that may not be immediately apparent through a simple text comparison can be uncovered, thereby reducing time consumed in finding conflicts manually.
\end{definition}	


\subsubsection*{Design Phases}\label{design_phases}
To provide a comprehensive overview of the design phases, this section explains each step of the process, from initial setup to final evaluation, using practical examples.
\subsubsection*{Step 1: Data Preparation}\label{conflict_design_step_1}
As primary input, we receive a graph-based model generated by the Doccano tool, which represents the refined and annotated dataset for the recognition of \emph{entities}, \emph{actions}, \emph{persons} and \emph{benefits} of USs \cite{arulmohan2023extracting}.

The datasets have the JSON format, the structure of which is very important in the Java classes \textit{RuleCreator}, \textit{ReportExtractor}, and \textit{Evaluation}. Therefore, understanding the JSON format provided is needed for the further procedure.

Each JSON file for a backlog dataset contains a JSON-array in which each US entry is defined as a JSON-object. Listing \ref{list:conflict_desing_json_format} illustrates the format used for the US entry.
\begin{MyListing}
	\paragraph{}
	\hrule
	\centering
	\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/conflict_json_format.json}
	\caption{The JSON format of each US entry in JSON file}\label{list:conflict_desing_json_format}
	\hrule
\end{MyListing}
A US in this JSON format contains the following entries:
\begin{itemize}
	\item ID: is the actual US identifier
	
	\item Text: this is the text from US
	
	\item Entities: is an array of JSON objects with the following entries:
	\begin{itemize}
		\item ID: serves as a reference number for the entry
		
		\item Label: is the name of the entry, which can be "PID", "Persona", "Entity", "Action" or "Benefit"
		
		\item Start\_offset: refer to the position of the first character of the word within the entire text
		
		\item End\_offset: this is the position immediately after the last character of the word within the entire text
	\end{itemize}
	\item Relations: is an array of JSON objects with the following entries:
	\begin{itemize}
		\item ID: serves as a reference number for the entry
		
		\item From\_ID: is the entry ID of the first element that occurs in the relation
		
		\item To\_ID: is the entry ID of the second element that occurs in the relation
		
		\item Type: is the type of relation, which can be "Triggers", "Targets", or "Contains"
	\end{itemize}
	As relation, each \emph{Persona} is linked to each \emph{Action} as \emph{Trigger} relation, each \emph{Action} is linked to each \emph{Entity} as \emph{Target} relation and each \emph{Entity} is linked to each \emph{Entity} implying a \emph{Contains} relation.
\end{itemize}
\subsubsection*{Step 2: Creation of Rules}\label{conflict_design_step_2}
Step 2 of the design involves a central process in which the US data structured in JSON files is transformed into transformation rules using the Henshin API. This involves the creation of an Ecore meta-model that represents the structure of the data we are working with, followed by the generation of Henshin transformation rules using RuleCreator class.
\subsubsection*{Creating Ecore Meta-Model}\label{design_workflow_ecore}
To be able to create rules in Henshin, an Ecore (meta)-model should be available. Ecore is the core (meta)-model at the heart of the EMF (Eclipse Modelling Framework). It enables the formulation of other models by utilising its constructs.

Accordingly, we create an Ecore meta-model as shown in Figure \ref{fig:design_ecore_meta_model}, which is inspired by the meta-model shown in Figure \ref{fig:conceptual_metamodel} and corresponds to the JSON-objects in the JSON-file as follows:
\begin{itemize}
	\item \textit{Persona} as a class in the meta-model corresponds to the JSON-object \enquote{Persona} in the JSON-file.
	\item \textit{Entity} as an abstract class, from which \textit{Primary/Secondary Entity} inherits as a class in the meta-model, corresponds to the JSON-object \enquote{Entity}, which contains two JSON-arrays, namely \enquote{Secondary/Primary Entity} in the JSON-file.
	\item \textit{Action} as an abstract class and \textit{Primary/Secondary Action} as an inherited class in the meta-model correspond to the JSON-object \enquote{Action}, which contains two JSON-arrays, namely \enquote{Secondary/Primary Action} in the JSON-file.
	\item \textit{Benefit} as a class in the meta-model, which also has an attribute called \enquote{text} that corresponds to the JSON-object \enquote{Benefit} in the JSON-file.
	\item \textit{Story} as a class in the meta-model that contains text from US, which also has an attribute called \enquote{text} that corresponds to the JSON-object \enquote{Text} in the JSON-file.
	\item Abstract class \textit{NamedElement} has attribute \textit{name}, which Primary/Secondary Action/Entity inherit from it, which corresponds to the value of \textit{Primary/Secondary Action/Entity} in JSON-file.
	\item \textit{Edge} with the name \textit{triggers} between Persona and Primary Action in the meta-model, which corresponds to the JSON-array \enquote{Triggers}, where each JSON-array in it contains a pair, the first element corresponding to the \textit{Persona} and the second to the \textit{Primary Action}.
	\item \textit{Edge} named \textit{targets} between Primary/Secondary Action and Primary/Secondary Entity in the meta-model, which corresponds to the JSON-array \enquote{Targets}, where each JSON-array has a pair, the first element corresponding to \enquote{Primary/Secondary Action} and the second element corresponding to \enquote{Primary/Secondary Entity}.
	\item \textit{Edge} named \textit{contains} between Primary/Secondary Entity and itself in the meta-model, which corresponds to the JSON-array \enquote{Contains}, where each JSON-array in it has a pair where the first element corresponds to \enquote{Primary/Secondary Entity} and the second element corresponds to \enquote{Primary/Secondary Entity}.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{ecore_metamodel}
	\caption{Ecore meta-model inspired by Mosser et al. \cite{mosser2022modelling}}\label{fig:design_ecore_meta_model}
\end{figure}
\subsubsection*{Creating Rules}\label{design_workflow_rule_creator}
With the identified USs in the the JSON-file, we generate rules with the Henshin package \textit{org.eclipse.emf.henshin.model.compact}, which is responsible for the creation of \textit{transformation rules} and their \textit{classes}, \textit{attributes}, \textit{edges} and annotates them with \textless\emph{Delete}\textgreater, \textless\textit{Create}\textgreater or\textless\textit{Preserve}\textgreater, which are vital for the CDA tool to recognise the redundant pairs.

To generating rules we create a package named \textit{org.henshin.backlog.code.rule} and specially the class \textit{RuleCreator} which used following classes\footnote{https://wiki.eclipse.org/Henshin/Compact\_API}:
\begin{itemize}
	\item \textit{org.eclipse.emf.henshin.model.compact.CModule}: CModule class can import elements from an Ecore file to use them in the transformation process responsible for linking the Ecore meta-model to the Henshin-file to be created.
	\item \textit{org.eclipse.emf.henshin.model.compact.CRule}: Once we have a CModule, we can specify transformation rules with the CRule class and create them.
	\item \textit{org.eclipse.emf.henshin.model.compact.CNode}: Now that we have a transformation rule, we want to fill this rule with nodes, edges and attributes. To create a node within a transformation rule, we need the CRule class. To create an edge we need to reference two nodes together. The default action when specifying a node, edge or an attributes is the \textless\emph{preserve}\textgreater action. We can also specify a different action when we create a node or an edge, for example \textless\emph{delete}\textgreater or\textless\emph{create}\textgreater.
	\item org.henshin.backlog.code.rule.RuleCreator: We implement \textit{RuleCreator} class that creates a rule with annotated nodes, edges and attributes based on a JSON-file as input and a Henshin-file containing all rules as output, where each rule and its members(nodes, attributes, edges) correspond to the individual US and their JSON-objects/arrays in the JSON-file. 
	
	The most important design decision of this class is the way nodes, attributes and edges are annotated in order to be able to apply conflict and dependency analysis (CDA) for rules stored as a Henshin file.
	
	We decided to annotate the \enquote{name} attribute of all Primary/Secondary Actions/Entities and their associated edges including \enquote{targets}, \enquote{triggers} and \enquote{contains} as \textless delete\textgreater  action. 
	
	The main goal is to increase the probability of identifying US-pairs characterised by matching names of \textit{action} nodes and \textit{entity} nodes in conjunction with an edge called \textit{targets}. This congruence serves as a basic criterion for identifying potentially redundant US-pairs and simplifies the process of redundancy detection in the context of US analysis.
\end{itemize}
\begin{example}
	Listing \ref{list:design_json_user_story_12} shows the JSON format in relation to user\_story\_12 and Figure \ref{fig:desing_rule_user_story_12} shows the application of the RuleCreator class in this US, which is a transformation rule where the targets and the associated contains relationships are annotated as a \textless Delete\textgreater action and the rest of the nodes and edges are annotated as a \textless Preserve\textgreater action.\\\\
	Text of US is:
	user\_story\_12: "\#G03\# As a Staff member, I want to Assign an Application for Detailed Review, so that I can review the for compliance and subsequently approved or denied."
	%Considering the backlog dataset as shown in Listing \ref{list:backlog_g03}:
	\begin{MyListing}
		\paragraph{}
		\centering
		\includegraphics[scale=0.8]{Listing/json_user_story_12.png}
		\caption{JSON entities Related to user\_story\_12}\label{list:design_json_user_story_12}
	\end{MyListing}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.6]{rule_user_story_12}
		\caption{Generated transformation rule related to user\_story\_12 using RuleCreator class}\label{fig:desing_rule_user_story_12}
	\end{figure}
	As we can see, the "targets" edges and their direct relationships ("triggers" and "contain", if any) are also annotated as \textless delete\textgreater, which is very important to find redundant elements with the CDA tool.
\end{example}
\begin{example}
	Listing \ref{list:design_json_user_story_39} shows the JSON entities related to user\_story\_39 and Figure \ref{fig:design_rule_user_story_39} shows transformation rule generated by RuleCreator class.\\\\
	Text of US is:
	user\_story\_39: "\#G03\# As a Plan Review Staff member, I want to Review Plans, so that I can review them for compliance and either approve, or fail or deny the plans and record any conditions, clearances, or corrections needed from the Applicant."
	\begin{MyListing}
		\paragraph{}
		\centering
		\includegraphics[scale=0.8]{Listing/json_user_story_39.png}
		\caption{JSON entities Related to user\_story\_39}\label{list:design_json_user_story_39}
	\end{MyListing}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.43]{rule_user_story_39}
		\caption{Generated transformation rule related to user\_story\_39 using RuleCreator class}\label{fig:design_rule_user_story_39}
	\end{figure}
	Attribute "Plan", as we can see, it appears both in the main part as a primary entity and in the benefit part as a secondary entity, forming the various relationships as targets and contains.
\end{example}
\begin{example}
	Listing \ref{list:desing_json_user_story_51} shows the JSON entities related to user\_story\_51 and Figure \ref{fig:desing_rule_user_story_51} shows transformation rule generated by RuleCreator class.\\\\
	Text of US is:
	user\_story\_51: "\#G03\# As an Enforcement Staff member, I want to Issue a Notice of Violation, so that I can provide formal communication to the responsible party."
	\begin{MyListing}
		\paragraph{}
		\centering
		\includegraphics[scale=0.8]{Listing/json_user_story_51.png}
		\caption{JSON entities Related to user\_story\_51}\label{list:desing_json_user_story_51}
	\end{MyListing}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.55]{rule_user_story_51}
		\caption{Generated transformation rule related to user\_story\_51 using RuleCreator class}\label{fig:desing_rule_user_story_51}
	\end{figure}
	Last but not least, we have determined that the secondary entity "responsible party" has neither a target nor a contains relationship. Therefore, it is not annotated as \textless delete\textgreater, but as \textless preserve\textgreater. This is due to the fact that some phrases have been identified as entities in the Doccano tool, but their relationship is not annotated at all, which is problematic for analysing redundancy.
\end{example}
\subsubsection*{Step 3: Conflict and Dependency Analysis}\label{step_3}
After the rules and the corresponding henshin file have been created by the RuleCreator class, we are now able to pass them to the conflict and dependency analysis (CDA) to find potential redundancy pairs.

Since the analysis of conflicts and dependencies related to the \textit{attribute} is not yet considered in the CDA API \footnote{https://wiki.eclipse.org/Henshin/conflict\_and\_Dependency\_Analysis}, we decided to use the user interface (UI) of the CDA extension of Henshin, which supports analysis of conflict and dependencies of rules through the interactive use of CDA.

To apply CDA to Henshin files, we just need to right-click on the Henshin file and select \textit{Henshin} -\textgreater\textit{ conflict and Dependency Analysis} from the context menu as shown in Figure \ref{fig:henshin_context_menu}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{henshin_context_menu}
	\caption{Applying CDA to the selected Henshin file}\label{fig:henshin_context_menu}
\end{figure}
A user interface then appears, prompting to select the rule sets to be analysed and the type of analysis. We then select as \enquote{\textit{First}} and \enquote{\textit{Second} \textit{Rules}}, all rules related to USs. Additionally, as the type of analysis we select \enquote{\textit{conflicts}} as illustrated in Figure \ref{fig:select_rules}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{select_rules}
	\caption{CDA user interface: Selection of rules and type of analysis}\label{fig:select_rules}
\end{figure}
On the next page of the CDA UI shown in Figure \ref{fig:select_granularity}, we specify the depth of analysis that we use with \enquote{\textit{Fine granularity}} when selecting \enquote{\textit{Create a complete result table}} and \enquote{\textit{Create an abstract result table}}. 

Fine granularity provides a detailed examination of each conflicting rule by listing all conflict reasons. Unlike coarse granularity, which focuses only on minimal conflict reasons, fine granularity includes both minimal and more general conflict reasons. The binary granularity, where simple conflicting rule pairs are listed, may be too simple for complex systems where understanding the nature of the conflict is essential for the solution. 
We choose \enquote{Fine granularity} as the depth of analysis due to the fact that it shows all conflict reasons for each conflicting rule pair. This allows for a deeper understanding of how different model fragments contribute to conflicts.

A conflict reason is a model fragment whose presence leads to a conflict. General conflict reasons result from different combinations of minimal conflict reasons\cite{cda_api}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{select_granularity}
	\caption{CDA user interface: Selection of report granularity}\label{fig:select_granularity}
\end{figure}
During the execution of the CDA analysis, the rule pairs is analysed and a conflict analysis is performed. Once the calculation is complete, the results are listed in the \enquote{CDA} -\textgreater \enquote{Result window}, as shown in Figure \ref{fig:cda_report}. The top entry shows the granularity, which in our case is \enquote{Fine}. These entries contain the rule pairs that conflict with each other. Each rule pair contains a number of conflict reasons.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{cda_report}
	\caption{CDA report with fine granularity}\label{fig:cda_report}
\end{figure}
Figure \ref{fig:cda_report_in_project_dir} shows how the data is saved in the project tree view. The results directory is created in the directory containing the Henshin that was used for the analyses. The new folder name is the date and time at which the analysis was performed. In contrast to the \enquote{\textit{CDA/Results}} view, this folder contains all conflict reasons and atoms together in a rule pair directory.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{cda_report_in_project_dir}
	\caption{Saving CDA results data in the project structure view}\label{fig:cda_report_in_project_dir}
\end{figure}
For each conflict reason, there is a \enquote{\textit{minimal-model.ecore}} file, that contains packages in which various conflict elements such as \enquote{attributes} and \enquote{references} (edges) are mapped together and displayed in different packages.

Figure \ref{fig:minimal_model_packages} shows the representation of the conflicting attributes and references. An attribute has the property of changing the value and is represented by an arrow \enquote{-\textgreater}. The attribute from the first rule is separated from the second rule by an underscore, just as with the nodes.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{minimal_model_packages}
	\caption{Representation of redundant attributes and references in \textit{minimal-model.ecore} file}\label{fig:minimal_model_packages}
\end{figure}
\begin{example}
	To illustrate this step, we also pass the transformation rules created in step 2, which reflect three USs (user\_story\_12/39/51), to the CDA tool and selecting "conflicts" as conflict type to calculate with "fine granularity" as the depth of analysis.\\
	Once the calculation is complete, the results are listed in the "CDA" -\textgreater "Results Window" as shown in Figure \ref{fig:step_3_cda_report}. Figure \ref{fig:step3_cda_report_project_tree_view} shows how the data is saved in the project's tree view, which contains all conflict reasons and atoms together in a rule pair directory.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{step_3_cda_report}
		\caption{CDA report in relation to three transformation rules with "conflicts" as the type to be calculated and "fine granularity" as the depth of the analysis}\label{fig:step_3_cda_report}
	\end{figure}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{step3_cda_report_project_tree_view}
		\caption{Saving CDA results data in the project's tree view}\label{fig:step3_cda_report_project_tree_view}
	\end{figure}
	As we can see, the CDA tool has only found redundancy between user\_story\_12 and user\_story\_39. This is because there are no redundant clauses between two USs (user\_story\_12 and user\_story\_39) and user\_story\_51.\\
	Regarding the redundant elements specifically, we can refer to the created file "minimal-model.ecore", which is located in the tree view of the project under each conflict reason. Figure \ref{fig:step3_cda_report_project_tree_view} show the minimal-model.ecore file related to user\_story\_12 and user\_story\_39.\\\\
	The file Minimal-model.ecore, which refers to user\_story\_12 and user\_story\_39, is divided into packages, with each package containing different matches of redundant elements. If there is a redundancy between two elements, this is explicitly indicated by a hash symbol (\#). Figure \ref{fig:step3_cda_package} illustrates the redundant elements found in each package.\\\\
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{minimal_model_packages}
		\caption{Representation of the redundant elements in each package within \textit{minimal-model.ecore} file}\label{fig:step3_cda_package}
	\end{figure}
	For example, the attribute "name" with the value "review" in "Secondary Action" and the attribute "targets" with the value "compliance" in "Secondary Entity" are labelled as redundant (both with a hash symbol). The reference to "targets" is also marked as redundant (with a hash symbol), which means that the reference (targets) from "Secondary Action" to "Secondary Entity" is also redundant between USs, which is a very important criterion for finding redundancy correctly.
\end{example}
\subsubsection*{Step 4: Report Extraction}\label{step_4}
To create a lightweight report for the group or individual in question, we need to extract the key information from the CDA report, e.g. redundancy US-pair, redundancy clauses, count of redundancy clauses in each part of the US (main or benefit part), and create a report as a text file with the following information:
\begin{itemize}
	
	\item A table of potential redundant pairs with the number of total redundancy clauses.
	
	\item Founded potential redundant US-pairs.
	
	\item Redundancy words and clauses of founded US-pairs. Clauses consisting of two words that have one of the relationships triggers, targets or contains.
	
	\item Text of US-pairs whose redundancy words are marked with a hash symbol (\#).
	
	\item Parts of the sentence in which words and clauses are found.
\end{itemize}
\subsubsection*{Structure of a US}
The delineation and checking of redundancy clauses within USs requires a methodical approach, especially when distinguishing between the main and the benefit part of a US-pair. This distinction is crucial to ensure that redundancy identifications within one part are not mistakenly transferred to the other. Consequently, the analytical framework comprises three conditions, each of which specifies its own methodology for case processing:
\begin{itemize}
	\item Presence of benefit in both USs of the redundancy-pair: if a benefit is identifiable in each US of the pair, a process of separation is used to split the main content from the benefit parts. After this separation, a targeted search for redundancy clauses is carried out only within the main part, whereby identified redundancies are annotated with a hash symbol (\#). This process is repeated for the benefit parts to ensure a thorough check and marking of redundancies within each individual part.
	
	\item Exclusive presence of a benefit in one US of the redundancy-pair: In scenarios where only one US of the pair contains a benefit part, the analysis is limited to the main part of both USs. The aim remains the identification and annotation of redundancy clauses within this part. The lone benefit part remains in its original state and is excluded from the redundancy check.
	
	\item Absence of benefit parts in both USs of the pair: If neither of the two USs of the redundancy-pair contains a benefit part, the focus shifts completely to the main parts. The investigation is designed to highlight redundancy clauses within these parts, whereby the benefit parts are not taken into account due to their non-existence.
	
\end{itemize}
This structured and segmented approach ensures precise and efficient identification of redundancy clauses within the USs, optimising the clarity and effectiveness of textual report.
\begin{example}
	After the CDA directory for user\_story\_12 and user\_story\_39 is created by CDA tool graphic interface (UI), we pass the location of directory into ReportExtractor class and in order to extracting the important information and save it into the texual report as well as JSON report.
	Listing \ref{list:textual_report_sample} illustrates the example of the textual report. In this case, the report only contains one US-pair.
	\begin{MyListing}
		\paragraph{}
		\centering
		%\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/TextualReportSample.txt}
		\includegraphics[scale=0.7]{Listing/TextualReportSample.png}
		\caption{Example of generated textual report for one US-pair}\label{list:textual_report_sample}
	\end{MyListing}	
	As we can see, the text report consists of a 2 x 2 table whose first column and first row are US identifiers, and the numbers inside the table are the total number (benefit part + main part) of redundancy elements between two USs; secondly, the redundant clauses related to the redundant US-pair are listed; thirdly, the text of US whose redundant phrases are marked with a hash symbol; finally, the part of the clauses in which redundant elements occur is displayed.	
\end{example}
For further evaluation purposes and easy export of the report to another platform such as Excel, a JSON report is created that collects the information about redundant US-pairs separately in a JSON object with the following entries:
\begin{itemize}
	\item Potential Redundant User Stories:  which has stored the US-pair identifier(e.g. "user\_story\_12\_AND\_user\_story\_39"). 
	
	\item Status: consisting of "Main/Beneift Part Redundancy Clauses" and "Total Redundancy Clauses", which store the count of redundancy clauses in the main and benefit part as well as in the total part of the US.
	
	\item Entity: which can consist of a "Secondary/Primary Entity" and stores the founded redundant entities.
	
	\item Common Targets/Contains: which consists of the "Main Part" and "Benefit Part" entries and only stores the targets/contains relationships that are common between the USs in a particular part of the USs. For example, if there are common redundant targets in the main part of the USs, these are included in the "Main Part" entity of the "Common Targets".
	
	\item Text: consisting of two entries, namely "First UserStory" and "Second UserStory", in which the text of the US-pair whose hash symbol has already been applied in redundant phrases is stored.
	
	\item Project Number: stores the number of the Project(e.g. "G03").
	
	\item Part of Sentence: consists of the entries "First UserStory" and "Second UserStory", in which the part of the US sentences containing redundant clauses is stored.
	
	\item All Targets/Contains: which consists of the "Main Part" and "Benefit Part" entries and stores the whole targets/contains relationships that are occurred in the particular part of the USs.
	
\end{itemize}
\begin{example}
	Listing \ref{list:json_report_sample} illustrates the example of the JSON report regarding user\_story\_12 and user\_story\_39.
\end{example}
\begin{MyListing}
	%\paragraph{}
	
	\centering
	%\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/TextualReportSample.txt}
	\includegraphics[scale=0.7]{Listing/JSONReportSample.png}
	\caption{Example of generated JSON report for one US-pair}\label{list:json_report_sample}
	
\end{MyListing}	
\subsubsection*{Step 5: Report Evaluation}
The Evaluation class, part of the \textit{org.henshin.backlog.code.evaluation} package, was developed to determine the level of redundancy in USs based on JSON reports. This class provides methods to evaluate whether two USs are either fully or partially redundant, analysing different application components of these USs.

The evaluation process involves a complex logic to determine whether USs are redundant. This includes:
\begin{itemize}
	\item Checking whether the arrays are empty or contain similar elements.
	\item Comparing the individual elements in the arrays for both USs to determine if they fully match (full redundancy) or if they have some common elements (partial redundancy).
\end{itemize}
\begin{example}
	For the two US-pairs of dataset G03, we apply the evaluation class to determine whether there is redundancy in the main or benefit part, and if so, what type of redundancy is recognised(full or partially).\\\\
	As shown in Listing \ref{list:json_evaluation}, four entries are added to the JSON report, namely "Main Partially Redundant", "Benefit Part Fully Redundant",
	"Main Part Fully Redundant", "Benefit Partially Redundant" as "Status" which their value is whether true or false. 
	\begin{MyListing}
		\paragraph{}
		
		\centering
		%\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/TextualReportSample.txt}
		\includegraphics[scale=0.7]{Listing/json_evaluation.png}
		\caption{Example of generated entries in JSON report regarding evaluation of level of redundancy in main or benefit part}\label{list:json_evaluation}
		
	\end{MyListing}	
	
	In the case of user\_story\_12 and user\_story\_39, the entry "Benefit Partially Redundant" was marked as \textit{true}, which means that US-pair in benefit parts are partially redundant.
	
\end{example}
The class performs these checks by iterating through the JSON arrays of Triggers, Targets and Contains and comparing each element with those in the common sections to determine redundancy.


\input{Section/Conflict_Implementation}