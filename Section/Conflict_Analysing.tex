\section{Analysing Conflict}\label{conflict}
In software development, especially in agile methods, conflict analysis is an important task to ensure the coherence and functionality of the system to be developed. A conflict is defined as a inconsistency that arises when two or more requirements, often encapsulated as USs, contradict each other. This section will introduce and define conflict analysis, focusing on the concept of \textit{content inconsistency} between USs.

The main objective of this analysis is to rationalise the software development workflow by semantically identifying conflicts between the USs within the backlog of a project.

A conflict of requirements arises when two or more USs show contradictions or inconsistencies. This can manifest itself in various forms, e.g. in the manipulation of the same resource by several USs at the same time, in overlapping functions or in conflicting conditions.

\begin{example}
	Considering following USs:\\\\
	user\_story\_13: "\#G03\# As a Staff member, I want to Apply a Hold, so that I can prevent progression through the workflow or other actions in the system until the issue is resolved."\\\\
	user\_story\_14: "\#G03\# As a Staff member, I want to Remove a Hold, so that I can allow progression through the workflow or other actions in the system now that the issue has been resolved."\\\\
	In this example, user\_story\_14 deletes a resource that is used by user\_story\_13. This means that user\_story\_13 cannot be applied at all if user\_story\_14 is executed first, which leads to a conflict between these USs.
\end{example}

In section \ref{conflict_requirement}, we present the requirements and functional needs that serve as input for the design phase in order to fulfil the requirements. In section \ref{conflict_desing} we explain the design decisions of the workflow shown in figure \ref{fig:conflict_operational_flow} and explain how the architecture is structured. 

\subsection{Requirements}\label{conflict_requirement}
In order to accomplish the analysis of conflicts in USs we try to address following functional requirements:
\begin{itemize}
	
	\item As a user, I want to perform semantic analysis on user stories within a specified project backlog, so that I can identify and address conflicts effectively.
	
	\item As a user, I want a report on the US-pairs that are conflicting in the main parts so I can change them if needed.
	
	\item As a user, I want to apply a filter to the conflict report to only show US-pairs that have the same resource (as entity) with different verbs (as action), so that the verbs are semantically contradictory (e.g. one US deletes a resource that another US is using, or deleting or one US creates a resource that another US prohibits).
	
	%\item As a user, I want to mark found redundancy clauses as Triggers with a hash symbol (\#) and show those that have a redundancy in \enquote{Persona} (as a noun) and \enquote{Action} (as a verb) entries, so that I can better see if the persona in is also recognised as a redundancy.
	
	%\item As a user, I want to mark the containers (as Contains) with a hash symbol (\#) and display found resources as conflict element (as noun), so that I can better see whether the contained entity is also recognised as a conflict.
	
	\item As a user, I would like to have a conflict report that shows founded US texts in US-pairs and adds a hash symbol (\#) at the beginning and end of conflicting verbs and a noun (as a resource) as a marker, so that I can better recognise the verbs and noun that conflict in US-pairs.
	
	\item As a user, I want to see how many conflict US-pairs have been created in the main parts of the USs within a backlog, so that I can summarise conflict US-pairs founded on this basis for further statistical purposes.
	
	\item As a user, I want a table at the top of the conflict report that lists the US-pairs in conflict, so that I can quickly see all the US-pairs that have been founded.
	
\end{itemize}
To judge the operation of a system, we define following non-functional requirements:
\begin{itemize}
	\item Testability: The system should support automated test procedures to ensure that semantic analysis and conflict detection work correctly. It should include comprehensive test cases covering different scenarios, including edge cases, to verify the accuracy and reliability of conflict detection.
	
	\item Documentation: The system should include detailed documentation covering all aspects of functionality and setup. User manuals, API documentation and troubleshooting guides should be provided. 
	
	\item Performance: The system should perform the conflict analysis within a reasonable time frame, even with large project backlogs. It should be optimised so that it can process large volumes of data without any significant loss of performance.
	
	\item Scalability: The system should be scalable to handle an increasing the number of USs and larger project backlogs.
		
\end{itemize}

\subsection{Design}\label{conflict_desing}
This section describes the operational flow and architectural considerations that underpin the framework.
\subsubsection*{Design Overview}
To address the conflict detection requirements specified in Section \ref{conflict_requirement}, our system used the backlogs labelled with Doccano tool\footnote{https://github.com/ace-design/nlp-stories} generated by Mosser et al. as the primary input\cite{arulmohan2023extracting}. 

In order to apply conflict analysis, a preparation phase is required. In this phase, each verb should be classified using the VerbNet\footnote{\href{https://verbs.colorado.edu/verbnet}{https://verbs.colorado.edu/verbnet}} classification. Finally, each verb class annotated into four categories, namely \textit{Create, Delete, Preserve, or Forbid} (from now on) called \textit{action-rule}. To validate the action rules, a personal judgement is made for each verb in the context of the backlog by three evaluators as a final check, so that each person verifies the action rules for each verb and also comments his/her own interpretation. We then collect all personal judgements and combine all inconsistencies for each verb. Subsequently, we add action rules into the backlogs labelled with Doccano tool. 

After preparation, annotated USs are used to analyse conflicts between USs in the backlog using the \textit{ReportMaker} class.

Finally, a conflict assessment phase is initiated for further statistical purposes. 

Figure \ref{fig:conflict_operational_flow} illustrates how each step in this sequence is interconnected, with the output of one step feeding directly into the next. This diagram effectively demonstrates the toolchain and process workflow, highlighting how each tool transforms artefacts and contributes to the overall objective of conflict detection.
\begin{figure}[h]
	\centering 
	\includegraphics[scale=0.65]{conflict_operational_flow}
	\caption{Step-by-step visualisation of the tool chain and its inputs and outputs}\label{fig:conflict_operational_flow}
\end{figure}
\paragraph{Preparing the JSON-Format of the Primary Input}\label{conflict_workflow_preparing_json_format}
Since the annotated USs in the original JSON files did not separate entries such as Entity, Action, Text, Targets and Contains, it is not clear which element belongs to which part of the USs, which leads to ambiguity. Accordingly, we use a class called \textit{JSONTransformer} that separates entries based on their occurrence in both the main and benefit parts of the USs. It also specifies an identifier to assign a unique identifier to each US, which is stored in a JSON object called \textit{"US\_Nr"}. 
These additions improve the system's ability to distinguish and process individual USs within the analytical pipeline.

\paragraph{Extraction of text reports} Creating a text report aims to find conflicts between USs and highlight important information, such as identifying potentially conflicts US-pairs, the conflict reason, the resource (as noun) affected by actions (as verbs) causing the conflict, the texts of the main parts with the affected elements marked with \# and a tabulation of the potentially conflicting pairs.

\paragraph{Evaluating the reports} Once we have created the reports, we can now assess the correctness of the US-pairs reported as conflicts, i.e. whether the reported US-pairs really cause a conflict.

\subsubsection*{Software Architecture}\label{conflict_architectur}
In this section, we present the basic structures of our workflow and the discipline of creating such structures. Each structure comprises software elements, relations among them, and properties of both.
\begin{itemize}
	\item Annotated USs with Doccano Tool\footnote{https://github.com/doccano/doccano}: Mosser et al. used publicly available requirements from Dalpiaz et al.\cite{Dalpiaz2018} consisting of 19 product backlogs and 1,458 USs. The dataset is a raw archive of 19 text files, each containing one US per line. As there were no public expert-based annotations, Mosser et al. manually annotated the dataset using the Doccano tool for \textit{Named Entity Recognition}. Labels included persona, action, entity, benefit part and relations such as triggers, targets, and contains based on their domain meta-model.
	As artefact we receive a graph-based model with JSON format, which represents the refined and annotated dataset for the recognition of \emph{entities}, \emph{actions}, \emph{personas} and \emph{benefits} of USs \cite{mosser2022modelling}.
	
	\item Eclipse as IDE\footnote{https://eclipseide.org/}: Eclipse is an integrated development environment (IDE) used in computer programming. It contains a base work workspace and an extensible plug-in system for customizing the environment.
	We chose this IDE because it offers the Henshin tool specifically for model-based development.
	
	\item VerbNet as Verb Lexicon Resource\footnote{https://verbs.colorado.edu/verbnet/}: VN is the largest on-line network of English verbs that links their syntactic and semantic patterns. It is a hierarchical, domain-independent, broad-coverage verb lexicon with mappings to other lexical resource, such as WordNet\footnote{https://wordnet.princeton.edu/}, PropBank \footnote{https://propbank.github.io/}, and FrameNet \footnote{http://framenet.icsi.berkeley.edu/}. VerbNet is organized into verb classes extending Levin (1993) classes through refinement and addition of subclasses to achieve syntactic and semantic coherence among members of a class. Each verb class in VN is completely described by thematic roles, selectional preferences of the arguments, and frames consisting of a syntactic description and a semantic representation with subevent structure patterned on the Dynamic Event Model of Pustejovsky and Moszkowicz and Pustejovsky\cite{kipper2006extending}.
	
	\item JSONTransformer Class: This class is part of the \textit{org.henshin.backlogconflict.code.rule} package and is a key component of the software architecture designed for transforming primary input datasets in JSON format. It separates entries based on their occurrence in both the main and benefit parts of the USs. It also assigns a unique identifier to each US to facilitate tracking and management. 
	
	This transformation simplifies subsequent tasks for editing, analysing and conflict resolution by providing a clear structure for the USs and their components. The separation of main and benefit parts and the assignment of unique identifiers improves the manageability and traceability of USs within the system.
	
	\item Action Rule Reference Database: This component is essential for identifying conflicts between USs, especially conflicts arising from actions over common entities ( as resources). To achieve this, we categorise actions into four different groups: \textit{Preserve, Delete, Create and Prohibit}.
	
	The main purpose of the action rule reference database is to facilitate the translation of actions (in the form of verbs) found in USs into corresponding action-rules. This process involves several important steps:
	\begin{enumerate}
		\item Collection of Actions: We collect all actions (represented as verbs) from existing datasets and compile them into a CSV file. This file serves as comprehensive reference database.
		
		\item Contextual Translation: Each verb in the CSV file is translated into the corresponding action-rules related to its VerbNet class. 
	\end{enumerate}
	
	\item ActionsRulesCreator Class: The ActionsRulesCreator class is an integral part of the \textit{org.henshin.backlogconflict.code.rule} package. It leverages the action rule reference database to enhance the JSON transformation process by adding new \textit{target} JSON-Array entries. These entries consist of a set of triples: action, entity, and action-rules. The action-rules are derived from the reference database, ensuring consistent and accurate conflict detection and resolution.
	
	\item ReportMaker Class: This class developed within the \textit{org.henshin.backlogconflict .code.report} packages, its primary function is to identify conflicting US-pairs based on specific criteria and generate comprehensive reports on these conflicts. This class provides detailed information on the nature of the conflicts, making it an invaluable tool for maintaining system coherence and resolving inconsistencies. It performs the following key tasks:
	\begin{enumerate}
		\item Identification of Conflict US-pairs: The class analysis USs to identify pairs that conflict based on predefined criteria. These criteria may including conflicting actions, or inconsistencies in the USs.
		
		\item Detailed Conflict Reporting: Once conflicts are identified, the class generates detailed reports. These reports contain essential information such as the affected entity(as resource), potential conflicted actions, conflict reason, and the texts of main parts of the USs with marked elements with "\#".
		
		\item Tabular Summary: In addition to detailed conflict descriptions, the class also produces a tabular summary of all identified conflict pairs in a backlog, providing a quick overview of the conflict US-pairs.
	\end{enumerate}
	
	\item ReportExtractor Class: The class from the \textit{org.henshin.backlog.code.report} package is used in our software architecture to extract and format reports from the CDA-generated \textit{Minimal-Model.ECore} that contain all information about redundant US-pairs such as redundant elements with their name and type. It uses classes from the \textit{org.eclipse.emf.ecore} package to handle EMF (Eclipse Modelling Framework) resources that support the management and manipulation of \textit{Minimal-Model.ECore} data in a structured format.
	
	In operation, the ReportExtractor class reads minimal-model.ecore files containing detailed information about redundant US-pairs. It then processes this data to generate reports in both text and JSON formats, aiding in the systematic analysis of potential redundancies within the USs. This is facilitated through methods that dynamically read and interpret the JSON data, extracting key information such as actions, entities, and their interactions.
	
	\item Evaluation Class: The class is part of the \textit{org.henshin.backlog.code.evaluation} package, serves a critical function within our software architecture, focusing on the analysis and determination of redundancy between USs based on specified criteria. This class utilizes JSON processing to assess the overlap and redundancy between elements of USs, such as triggers, targets, and contains, which are vital for identifying potential redundancies in the USs.
	
	Key functionalities of this class include reading and interpreting JSON data, where it extracts detailed elements related to USs and evaluates them against redundancy criteria defined in its methods. The class's method \textit{evaluateRedundancyCriteria} is particularly central, as it processes the JSON objects representing USs to determine if they are fully or partially redundant based on the presence and matching of various JSON array elements like triggers, targets, and contains in the main and benefit parts of the USs.
	
\end{itemize}
Figure \ref{fig:conflict_technical_implementation} shows the architectural composition, highlighting the integral components and their user interface and artefacts.
\begin{figure}[h]
	\centering 
	\includegraphics[scale=0.6]{technical_implementation}
	\caption{Design phases}\label{fig:conflict_technical_implementation}
\end{figure}
Regarding conflict, some definitions are clarified:
\begin{definition}[\textbf{Main and Benefit Parts in User Story}]
	A user story (US) is divided into two distinct parts that collectively describe what the user wants, why they want it, and how it will benefit them. These are:
	\begin{itemize}
		\item The \textit{main part}, which is essential as it clearly and concisely summarises the persona, the intended functionality and the resources required to perform the action. This part usually has follow the format: \textit{"As a [persona], I want [what]"}. 
		
		Here, the persona helps contextualise the requirement by linking it to a user type, which promotes understanding and empathy. The intended functionality describes the action that the persona wants to perform or the function they need, providing a clear statement of the requirement.
		
		Specifying the resources required to perform the action helps with planning and resource allocation and ensures that the development team is aware of the tools, technologies and time required.
		
		\item The \textit{benefit part}, which formulates the potential benefit for the end user and typically begins with the phrase \enquote{so that}. This part of the US is used for justifying the need for a feature by explaining the value or improvement it brings to the user's experience. It connects the functionality directly to user satisfaction, efficiency, or productivity gains, making it easier for the development team to prioritize features based on their impact.
		
		It is worth noting that sometimes the benefit part may not exist in the structure of USs. In such cases, the main part can stand alone.
	\end{itemize}
\end{definition}
\newpage
\begin{definition}[\textbf{Conflict}]
	 %TODO: angepasste US
	Conflict refers to situations where two USs try to:	 
	\begin{itemize}
		\item delete a resource which another US are using
		\item delete a resource which another US also wants to delete
		\item create a resource which another US prohibits
	\end{itemize}
	$Notation$. Lowercase identifiers refer to single elements, and uppercase identifiers denote sets. 
	\\A user story $us$ is a 2-tuple $us = \langle m,b\rangle $ where:
	\begin{enumerate}
		\item A main $m$ is define a 6-tuple: \\\\$m = \langle p,A,E,Tr,Ta,Co\rangle $ \\\\where:
		
	\begin{itemize}
		\item $p$ is the persona.
		
		\item $A = \{ a_1,a_2,...\} $ is a set of actions.
		
		\item $E = \{e_1,e_2,...\}$ is a set of entities.
		
		\item $Tr = \{(p_1,a_1),(p_2,a_2),...\}$ is a set of trigger references, each begin a pair of persona and action.
		
		\item $Ta = \{(a_1,e_1,R_1),(a_2,e_2,R_2),...\}$ is a set of target references, each begin a triple of action, entity, and action-rules $R$.
		
		\item $Co = \{ (e_1,e_2,R_1),(e_*,e_*,R_2),... \}$ is a set of contain references, each begin a triple of entities and action-rules R.
		
		\item $R = \{preserve, create, delete, forbid\}$ are the rules applied to actions.
	\end{itemize}
		\item A benefit $b$ is define as a 4-tuple:\\\\ $b = \langle A,E,Ta,Co\rangle $\\\\ where: 
		\begin{itemize}
			\item $A = \{ a_1,a_2,...\} $ is a set of actions.
			 
			\item $E = \{e_1,e_2,...\}$ is a set of entities.
			
			\item $Ta = \{(a_1,e_1),(a_2,e_2),...\}$ is a set of target references, each begin a pair of action and entity.
			
			\item $Co = \{ (e_1,e_2),(e_*,e_*),... \}$ is a set of contain references, each begin a pair of entities. 
			
		\end{itemize}
	\end{enumerate}
	To denote that a syntactic operator, we add the subscript
	“syn”; for instance, $=_{syn}$ is syntactic equivalence which introduced by Lucassen et al. \cite{lucassen2016improving}.\\ Consider two USs:\\\\ $us_1 = \langle m_1,b_1\rangle $ where $m_1 = \langle p_1,a_1,e_1,tr_1,ta_1,co_1 \rangle$\\\\$us_2 = \langle m_2,b_2\rangle$ where $m_2 = \langle p_2,a_2,e_2,tr_2,ta_2,co_2 \rangle$ \\\\
	$us_1$ causes a conflict if:\\\\ The entity $e_1$ is an exact redundant of entity $e_2$, formally:\\ $isRedundant(e_1,e_2) \leftrightarrow e_1 =_{syn} e_2$ and one of the following conditions holds:\\
	\begin{enumerate}
		\item $ta_1 = (e_1,a_1,"preserve")$ and $ta_2 = (e_2,a_2,"delete")$
		
		\item $ta_1 = (e_1,a_1,"create")$ and $ta_2 = (e_2,a_2,"forbid")$
		
		\item $ta_1 = (e_1,a_1,"delete")$ and $ta_2 = (e_2,a_2,"delete")$
	\end{enumerate}
	To comprehensively assess conflicts, it is important to consider not only the textual content but also the functional relevance of each action over entities within the USs. By categorizing actions into four groups, conflict that may not be immediately apparent through a simple text comparison can be uncovered, thereby reducing time consumed in finding conflicts manually.
\end{definition}	
\begin{definition}[\textbf{Full Redundancy}]
	A US-pair are fully redundant in their main parts if
	\begin{itemize}
		\item there are redundant target references $Ta_2$ in $m_2$ for all target references $Ta_1$ in $m_1$, so that $Ta_1$ and $Ta_2$ are redundant, and vice versa, and
		
		\item there are redundant trigger references $Tr_2$ in $m_2$ for all trigger references $Tr_1$ in $m_1$, so that $Tr_1$ and $Tr_2$ are redundant, and vice versa, and
		
		\item there are redundant contain references $Co_2$ in $m_2$ for all contain references $Co_1$ in $m_1$, so that $Co_1$ and $Co_2$ are redundant, and vice versa.
		
	\end{itemize}
	A US-pair are fully redundant in their benefit parts if
	\begin{itemize}
		\item there are redundant target references $Ta_2$ in $b_2$ for all target references $Ta_1$ in $b_1$, so that $Ta_1$ and $Ta_2$ are redundant, and vice versa, and
		
		\item there are redundant contain references $Co_2$ in $b_2$ for all contain references $Co_1$ in $b_1$, so that $Co_1$ and $Co_2$ are redundant, and vice versa.
		
	\end{itemize}
	A US-pair was categorised as "full redundancy" in the main/benefit part if all occurring clauses in the main/benefit part consisting of triggers (for the main part only), targets and contains were syntactically identical.\\\\
	This means that in each pair, the wording, order and structure of the clauses relating to the triggers and targets must be identical to fall into this category. The check also extends to the "contain" elements, if these are present, to ensure that these also match perfectly and without deviations.\\\\
	Full redundancy in the main part means that the redundant stories do not provide additional information and can be consolidated or eliminated without compromising the completeness or operational integrity of the system specifications.\\\\
	On the other hand, having this level of redundancy in the benefit part means that USs achieve the same goal; they can be categorised in the same group. This means that if the end results or benefits described by the USs are identical, regardless of the different actions/entities or triggers that lead to these results, the USs effectively serve the same purpose within the system.
\end{definition}
\begin{example}
	For example, the following US-pair is identified as \enquote{Full Redundancy} in the main part:
	
	\textit{user\_story\_01:} \enquote{\#g14\# as a \#publisher\#, i want to \#publish\# a \#dataset\#, so that i can view just the dataset with a few people.}
	
	\textit{user\_story\_02:} \enquote{\#g14\# as a \#publisher\#, i want to \#publish\# a \#dataset\#, so that i can share the dataset publicly with everyone.}\\\\
	All existing clauses associated with the main elements in this scenario - in particular the trigger (where "Publisher" is the persona and "Publish" is the action) and the targets (where "Publish" is the action and "Dataset" is the entity) - are exactly the same in the main parts of the USs. This complete similarity shows that there is "full redundancy" in the main parts of these USs.
\end{example}
\begin{example}
	Following US-pair is identified as \enquote{Full Redundancy} in \enquote{Benefit} part:
	
	\textit{user\_story\_02:} \#g05\# as a data publishing user, I want to be able to edit the model of data I have already imported, so that I can \#fix\# \#bugs\# or \#make\# \#enhancements\# in the \#API\# built for my \#data\#.
	
	\textit{user\_story\_07:} \#g05\# as a data publishing user, I want to be able to edit the data source of data I have already imported, so that i can \#fix\# \#bugs\# or \#make\# \#enhancements\# in the \#API\# built for my \#data\#.
	
	As we can see, targets(["fix", "bugs"], ["make", enhancements"]) and contains (["API", "enhancements"], ["data","API"]) in the benefit part are identical between USs, therefore, we have \enquote{Full Redundancy} in the \enquote{Benefit} parts.
\end{example}
Last but not least, sometime there are words in the annotated USs that are not labelled as a reference (trigger, target or contain) by the Doccano tool at all, which leads to a full redundancy being incorrectly recognised instead of a partial.
\begin{example}
	Considering following US-pair:
	
	user\_story\_26: \#g05\# as an \#api user\#, i want to be able to \#understand\# if a \#user\# is an administrator, so that [...].
	
	user\_story\_25: \#g05\# as an \#api user\#, i want to be able to \#understand\# if a \#user\# is a publisher, so that [...].\\\\
	In this example, full redundancy is found due to the redundant targets and triggers in the main part, which is not entirely correct. This is because there are some phrases that do not appear as contain or target in any reference, such as: "administrator" (user\_story\_26) and "publisher" (user\_story\_25).
\end{example}
\begin{definition}[\textbf{Partial Redundancy}]
	A US-pair are partially redundant in their main parts if there is a target reference $ta_1$ in $m_1$ and a target reference $ta_2$ in $m_2$, so that $ta_1$ and $ta_2$ are redundant.\\\\
	A US-pair are partially redundant in their benefit parts if there is a target reference $ta_1$ in $b_1$ and a target reference $ta_2$ in $b_2$, so that $ta_1$ and $ta_2$ are redundant.\\\\
	Note that in this definition, a US-pair that is fully redundant in the main part (or in the benefit part) is also partially redundant in the main part (or in the benefit part).\\\\
	Partial redundancy in USs occurs when only certain clauses, such as targets, have significant overlap but are not completely identical. This means that while there are shared aspects such as targets between USs, other clauses such as triggers or contains may not overlap, indicating an incomplete match. Such a scenario indicates that there is substantial, but not fully, a match between the USs.
	%	Was found when only some clauses were redundant in either the Main or Benefit parts, while others were unique. This indicates overlapping clauses, but not full redundancy.
\end{definition}
\begin{example}
	Following US-pair is identified as \enquote{Partial Redundancy} in main part:
	
	\textit{user\_story\_09:} \#g04\# as a \#user\#, I want to be able to \#view\# a \#map display\# of the public recycling bins around my \#area\#.
	
	\textit{user\_story\_10:} \#g04\# as a \#user\#, I want to be able to \#view\# a \#map display\# of the special waste drop off sites around my \#area\#.
	
	As we can see, there are some redundancy clauses such as Triggers (["user", "view"]), Targets (["view", "map display"]) and Contains(["map display", "area"]) between the USs. There are also clauses such as Contains (["public recycling bins"] vs. ["hazardous waste collection points"]) that are distinct elements justifying the maintaining of separate USs. We therefore assess this as "Partial redundancy" in the "Main part".
\end{example}
\begin{example}
	Following US-pair is identified as \enquote{Partial Redundancy} in benefit part:
	
	\textit{user\_story\_17:} \#g03\# as a staff member, I want to manage approved proffers, so that I can \#ensure\# \#compliance\# with and satisfaction of the proffer in the future.
	
	\textit{user\_story\_30:} \#g03\# as a staff member, I want to manage affidavits, so that I can \#ensure\# \#compliance\# with the requirements prior to the hearing.
	
	As we can see, there is a redundancy clause as targets (["ensure", "compliance"]) between the USs in the benefit part, but there is also a clause as contains (["proffer", "satisfaction"] vs ["requirements", "hearing"]), which leads us to evaluate this as \enquote{Partial Redundancy} in the \enquote{Benefit} part.
\end{example}
\subsubsection*{Design Phases}\label{design_phases}
To provide a comprehensive overview of the design phases, this section explains each step of the process, from initial setup to final evaluation, using practical examples.
\subsubsection*{Step 1: Data Preparation}\label{design_step_1}
As primary input, we receive a graph-based model generated by the Doccano tool, which represents the refined and annotated dataset for the recognition of \emph{entities}, \emph{actions}, \emph{persons} and \emph{benefits} of USs \cite{arulmohan2023extracting}.

The datasets have the JSON format, the structure of which is very important in the Java classes \textit{RuleCreator}, \textit{ReportExtractor}, and \textit{Evaluation}. Therefore, understanding the JSON format provided is needed for the further procedure.

Each JSON file for a backlog dataset contains a JSON-array in which each US entry is defined as a JSON-object. Listing \ref{list:desing_json_format} illustrates the format used for the US entry.
\begin{MyListing}
	\paragraph{}
	\hrule
	\centering
	\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/json_format.json}
	\caption{The JSON format of each US entry in JSON file}\label{list:desing_json_format}
	\hrule
\end{MyListing}
Mosser et al. have linked each \emph{Persona} to each \emph{Primary Action} as \emph{Trigger} relationships, each \emph{Primary Actions} to each \emph{Primary Entity} as \emph{Target} relationships and each \emph{Primary/Secondary Entity} to each \emph{Primary/Secondary Entity} implying a \emph{Contains} relationship\cite{arulmohan2023extracting}.

To interact with the entries in JSON file, we need to distinguish between the entries that are defined as JSON-objects, such as: {Text, Action, Entity, Benefit} and the entries that are defined as a JSON-array, such as: {Persona, Primary/Secondary Action, Primary/Secondary Entity, Triggers, Targets, Contains}.
\subsubsection*{Identifying USs in JSON-File}\label{desing_workflow_nummerize_us}
Annotated USs in each JSON file have no identifier. To distinguish USs, we use a Python script called \textit{nummerise\_us.py} \footnote{https://github.com/amirrabieyannejad/Redundancy\_Analysis/tree/main/Script/numberise\_us}, which receives JSON files as input and adds a JSON object named \enquote{US\_Nr} with an identifier as value (e.g. user\_story\_01) to each US and returns the JSON files as output.
Listing \ref{list:desing_json_format_sample} illustrates the added JSON object "US\_Nr" and its value in the JSON file.
\begin{MyListing}
	\paragraph{}
	\hrule
	\centering
	\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/json_format_sample.json}
	\caption{The JSON format with the additional JSON object "US\_Nr" and its value}\label{list:desing_json_format_sample}
	\hrule
\end{MyListing}
\subsubsection*{Step 2: Creation of Rules}\label{design_step_2}
Step 2 of the design involves a central process in which the US data structured in JSON files is transformed into transformation rules using the Henshin API. This involves the creation of an Ecore meta-model that represents the structure of the data we are working with, followed by the generation of Henshin transformation rules using RuleCreator class.
\subsubsection*{Creating Ecore Meta-Model}\label{design_workflow_ecore}
To be able to create rules in Henshin, an Ecore (meta)-model should be available. Ecore is the core (meta)-model at the heart of the EMF (Eclipse Modelling Framework). It enables the formulation of other models by utilising its constructs.

Accordingly, we create an Ecore meta-model as shown in Figure \ref{fig:design_ecore_meta_model}, which is inspired by the meta-model shown in Figure \ref{fig:conceptual_metamodel} and corresponds to the JSON-objects in the JSON-file as follows:
\begin{itemize}
	\item \textit{Persona} as a class in the meta-model corresponds to the JSON-object \enquote{Persona} in the JSON-file.
	\item \textit{Entity} as an abstract class, from which \textit{Primary/Secondary Entity} inherits as a class in the meta-model, corresponds to the JSON-object \enquote{Entity}, which contains two JSON-arrays, namely \enquote{Secondary/Primary Entity} in the JSON-file.
	\item \textit{Action} as an abstract class and \textit{Primary/Secondary Action} as an inherited class in the meta-model correspond to the JSON-object \enquote{Action}, which contains two JSON-arrays, namely \enquote{Secondary/Primary Action} in the JSON-file.
	\item \textit{Benefit} as a class in the meta-model, which also has an attribute called \enquote{text} that corresponds to the JSON-object \enquote{Benefit} in the JSON-file.
	\item \textit{Story} as a class in the meta-model that contains text from US, which also has an attribute called \enquote{text} that corresponds to the JSON-object \enquote{Text} in the JSON-file.
	\item Abstract class \textit{NamedElement} has attribute \textit{name}, which Primary/Secondary Action/Entity inherit from it, which corresponds to the value of \textit{Primary/Secondary Action/Entity} in JSON-file.
	\item \textit{Edge} with the name \textit{triggers} between Persona and Primary Action in the meta-model, which corresponds to the JSON-array \enquote{Triggers}, where each JSON-array in it contains a pair, the first element corresponding to the \textit{Persona} and the second to the \textit{Primary Action}.
	\item \textit{Edge} named \textit{targets} between Primary/Secondary Action and Primary/Secondary Entity in the meta-model, which corresponds to the JSON-array \enquote{Targets}, where each JSON-array has a pair, the first element corresponding to \enquote{Primary/Secondary Action} and the second element corresponding to \enquote{Primary/Secondary Entity}.
	\item \textit{Edge} named \textit{contains} between Primary/Secondary Entity and itself in the meta-model, which corresponds to the JSON-array \enquote{Contains}, where each JSON-array in it has a pair where the first element corresponds to \enquote{Primary/Secondary Entity} and the second element corresponds to \enquote{Primary/Secondary Entity}.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{ecore_metamodel}
	\caption{Ecore meta-model inspired by Mosser et al. \cite{mosser2022modelling}}\label{fig:design_ecore_meta_model}
\end{figure}
\subsubsection*{Creating Rules}\label{design_workflow_rule_creator}
With the identified USs in the the JSON-file, we generate rules with the Henshin package \textit{org.eclipse.emf.henshin.model.compact}, which is responsible for the creation of \textit{transformation rules} and their \textit{classes}, \textit{attributes}, \textit{edges} and annotates them with \textless\emph{Delete}\textgreater, \textless\textit{Create}\textgreater or\textless\textit{Preserve}\textgreater, which are vital for the CDA tool to recognise the redundant pairs.

To generating rules we create a package named \textit{org.henshin.backlog.code.rule} and specially the class \textit{RuleCreator} which used following classes\footnote{https://wiki.eclipse.org/Henshin/Compact\_API}:
\begin{itemize}
	\item \textit{org.eclipse.emf.henshin.model.compact.CModule}: CModule class can import elements from an Ecore file to use them in the transformation process responsible for linking the Ecore meta-model to the Henshin-file to be created.
	\item \textit{org.eclipse.emf.henshin.model.compact.CRule}: Once we have a CModule, we can specify transformation rules with the CRule class and create them.
	\item \textit{org.eclipse.emf.henshin.model.compact.CNode}: Now that we have a transformation rule, we want to fill this rule with nodes, edges and attributes. To create a node within a transformation rule, we need the CRule class. To create an edge we need to reference two nodes together. The default action when specifying a node, edge or an attributes is the \textless\emph{preserve}\textgreater action. We can also specify a different action when we create a node or an edge, for example \textless\emph{delete}\textgreater or\textless\emph{create}\textgreater.
	\item org.henshin.backlog.code.rule.RuleCreator: We implement \textit{RuleCreator} class that creates a rule with annotated nodes, edges and attributes based on a JSON-file as input and a Henshin-file containing all rules as output, where each rule and its members(nodes, attributes, edges) correspond to the individual US and their JSON-objects/arrays in the JSON-file. 
	
	The most important design decision of this class is the way nodes, attributes and edges are annotated in order to be able to apply conflict and dependency analysis (CDA) for rules stored as a Henshin file.
	
	We decided to annotate the \enquote{name} attribute of all Primary/Secondary Actions/Entities and their associated edges including \enquote{targets}, \enquote{triggers} and \enquote{contains} as \textless delete\textgreater  action. 
	
	The main goal is to increase the probability of identifying US-pairs characterised by matching names of \textit{action} nodes and \textit{entity} nodes in conjunction with an edge called \textit{targets}. This congruence serves as a basic criterion for identifying potentially redundant US-pairs and simplifies the process of redundancy detection in the context of US analysis.
\end{itemize}
\begin{example}
	Listing \ref{list:design_json_user_story_12} shows the JSON format in relation to user\_story\_12 and Figure \ref{fig:desing_rule_user_story_12} shows the application of the RuleCreator class in this US, which is a transformation rule where the targets and the associated contains relationships are annotated as a \textless Delete\textgreater action and the rest of the nodes and edges are annotated as a \textless Preserve\textgreater action.\\\\
	Text of US is:
	user\_story\_12: "\#G03\# As a Staff member, I want to Assign an Application for Detailed Review, so that I can review the for compliance and subsequently approved or denied."
	%Considering the backlog dataset as shown in Listing \ref{list:backlog_g03}:
	\begin{MyListing}
		\paragraph{}
		\centering
		\includegraphics[scale=0.8]{Listing/json_user_story_12.png}
		\caption{JSON entities Related to user\_story\_12}\label{list:design_json_user_story_12}
	\end{MyListing}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.6]{rule_user_story_12}
		\caption{Generated transformation rule related to user\_story\_12 using RuleCreator class}\label{fig:desing_rule_user_story_12}
	\end{figure}
	As we can see, the "targets" edges and their direct relationships ("triggers" and "contain", if any) are also annotated as \textless delete\textgreater, which is very important to find redundant elements with the CDA tool.
\end{example}
\begin{example}
	Listing \ref{list:design_json_user_story_39} shows the JSON entities related to user\_story\_39 and Figure \ref{fig:design_rule_user_story_39} shows transformation rule generated by RuleCreator class.\\\\
	Text of US is:
	user\_story\_39: "\#G03\# As a Plan Review Staff member, I want to Review Plans, so that I can review them for compliance and either approve, or fail or deny the plans and record any conditions, clearances, or corrections needed from the Applicant."
	\begin{MyListing}
		\paragraph{}
		\centering
		\includegraphics[scale=0.8]{Listing/json_user_story_39.png}
		\caption{JSON entities Related to user\_story\_39}\label{list:design_json_user_story_39}
	\end{MyListing}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.43]{rule_user_story_39}
		\caption{Generated transformation rule related to user\_story\_39 using RuleCreator class}\label{fig:design_rule_user_story_39}
	\end{figure}
	Attribute "Plan", as we can see, it appears both in the main part as a primary entity and in the benefit part as a secondary entity, forming the various relationships as targets and contains.
\end{example}
\begin{example}
	Listing \ref{list:desing_json_user_story_51} shows the JSON entities related to user\_story\_51 and Figure \ref{fig:desing_rule_user_story_51} shows transformation rule generated by RuleCreator class.\\\\
	Text of US is:
	user\_story\_51: "\#G03\# As an Enforcement Staff member, I want to Issue a Notice of Violation, so that I can provide formal communication to the responsible party."
	\begin{MyListing}
		\paragraph{}
		\centering
		\includegraphics[scale=0.8]{Listing/json_user_story_51.png}
		\caption{JSON entities Related to user\_story\_51}\label{list:desing_json_user_story_51}
	\end{MyListing}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.55]{rule_user_story_51}
		\caption{Generated transformation rule related to user\_story\_51 using RuleCreator class}\label{fig:desing_rule_user_story_51}
	\end{figure}
	Last but not least, we have determined that the secondary entity "responsible party" has neither a target nor a contains relationship. Therefore, it is not annotated as \textless delete\textgreater, but as \textless preserve\textgreater. This is due to the fact that some phrases have been identified as entities in the Doccano tool, but their relationship is not annotated at all, which is problematic for analysing redundancy.
\end{example}
\subsubsection*{Step 3: Conflict and Dependency Analysis}\label{step_3}
After the rules and the corresponding henshin file have been created by the RuleCreator class, we are now able to pass them to the conflict and dependency analysis (CDA) to find potential redundancy pairs.

Since the analysis of conflicts and dependencies related to the \textit{attribute} is not yet considered in the CDA API \footnote{https://wiki.eclipse.org/Henshin/conflict\_and\_Dependency\_Analysis}, we decided to use the user interface (UI) of the CDA extension of Henshin, which supports analysis of conflict and dependencies of rules through the interactive use of CDA.

To apply CDA to Henshin files, we just need to right-click on the Henshin file and select \textit{Henshin} -\textgreater\textit{ conflict and Dependency Analysis} from the context menu as shown in Figure \ref{fig:henshin_context_menu}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{henshin_context_menu}
	\caption{Applying CDA to the selected Henshin file}\label{fig:henshin_context_menu}
\end{figure}
A user interface then appears, prompting to select the rule sets to be analysed and the type of analysis. We then select as \enquote{\textit{First}} and \enquote{\textit{Second} \textit{Rules}}, all rules related to USs. Additionally, as the type of analysis we select \enquote{\textit{conflicts}} as illustrated in Figure \ref{fig:select_rules}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{select_rules}
	\caption{CDA user interface: Selection of rules and type of analysis}\label{fig:select_rules}
\end{figure}
On the next page of the CDA UI shown in Figure \ref{fig:select_granularity}, we specify the depth of analysis that we use with \enquote{\textit{Fine granularity}} when selecting \enquote{\textit{Create a complete result table}} and \enquote{\textit{Create an abstract result table}}. 

Fine granularity provides a detailed examination of each conflicting rule by listing all conflict reasons. Unlike coarse granularity, which focuses only on minimal conflict reasons, fine granularity includes both minimal and more general conflict reasons. The binary granularity, where simple conflicting rule pairs are listed, may be too simple for complex systems where understanding the nature of the conflict is essential for the solution. 
We choose \enquote{Fine granularity} as the depth of analysis due to the fact that it shows all conflict reasons for each conflicting rule pair. This allows for a deeper understanding of how different model fragments contribute to conflicts.

A conflict reason is a model fragment whose presence leads to a conflict. General conflict reasons result from different combinations of minimal conflict reasons\cite{cda_api}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{select_granularity}
	\caption{CDA user interface: Selection of report granularity}\label{fig:select_granularity}
\end{figure}
During the execution of the CDA analysis, the rule pairs is analysed and a conflict analysis is performed. Once the calculation is complete, the results are listed in the \enquote{CDA} -\textgreater \enquote{Result window}, as shown in Figure \ref{fig:cda_report}. The top entry shows the granularity, which in our case is \enquote{Fine}. These entries contain the rule pairs that conflict with each other. Each rule pair contains a number of conflict reasons.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{cda_report}
	\caption{CDA report with fine granularity}\label{fig:cda_report}
\end{figure}
Figure \ref{fig:cda_report_in_project_dir} shows how the data is saved in the project tree view. The results directory is created in the directory containing the Henshin that was used for the analyses. The new folder name is the date and time at which the analysis was performed. In contrast to the \enquote{\textit{CDA/Results}} view, this folder contains all conflict reasons and atoms together in a rule pair directory.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{cda_report_in_project_dir}
	\caption{Saving CDA results data in the project structure view}\label{fig:cda_report_in_project_dir}
\end{figure}
For each conflict reason, there is a \enquote{\textit{minimal-model.ecore}} file, that contains packages in which various conflict elements such as \enquote{attributes} and \enquote{references} (edges) are mapped together and displayed in different packages.

Figure \ref{fig:minimal_model_packages} shows the representation of the conflicting attributes and references. An attribute has the property of changing the value and is represented by an arrow \enquote{-\textgreater}. The attribute from the first rule is separated from the second rule by an underscore, just as with the nodes.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{minimal_model_packages}
	\caption{Representation of redundant attributes and references in \textit{minimal-model.ecore} file}\label{fig:minimal_model_packages}
\end{figure}
\begin{example}
	To illustrate this step, we also pass the transformation rules created in step 2, which reflect three USs (user\_story\_12/39/51), to the CDA tool and selecting "conflicts" as conflict type to calculate with "fine granularity" as the depth of analysis.\\
	Once the calculation is complete, the results are listed in the "CDA" -\textgreater "Results Window" as shown in Figure \ref{fig:step_3_cda_report}. Figure \ref{fig:step3_cda_report_project_tree_view} shows how the data is saved in the project's tree view, which contains all conflict reasons and atoms together in a rule pair directory.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{step_3_cda_report}
		\caption{CDA report in relation to three transformation rules with "conflicts" as the type to be calculated and "fine granularity" as the depth of the analysis}\label{fig:step_3_cda_report}
	\end{figure}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{step3_cda_report_project_tree_view}
		\caption{Saving CDA results data in the project's tree view}\label{fig:step3_cda_report_project_tree_view}
	\end{figure}
	As we can see, the CDA tool has only found redundancy between user\_story\_12 and user\_story\_39. This is because there are no redundant clauses between two USs (user\_story\_12 and user\_story\_39) and user\_story\_51.\\
	Regarding the redundant elements specifically, we can refer to the created file "minimal-model.ecore", which is located in the tree view of the project under each conflict reason. Figure \ref{fig:step3_cda_report_project_tree_view} show the minimal-model.ecore file related to user\_story\_12 and user\_story\_39.\\\\
	The file Minimal-model.ecore, which refers to user\_story\_12 and user\_story\_39, is divided into packages, with each package containing different matches of redundant elements. If there is a redundancy between two elements, this is explicitly indicated by a hash symbol (\#). Figure \ref{fig:step3_cda_package} illustrates the redundant elements found in each package.\\\\
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{minimal_model_packages}
		\caption{Representation of the redundant elements in each package within \textit{minimal-model.ecore} file}\label{fig:step3_cda_package}
	\end{figure}
	For example, the attribute "name" with the value "review" in "Secondary Action" and the attribute "targets" with the value "compliance" in "Secondary Entity" are labelled as redundant (both with a hash symbol). The reference to "targets" is also marked as redundant (with a hash symbol), which means that the reference (targets) from "Secondary Action" to "Secondary Entity" is also redundant between USs, which is a very important criterion for finding redundancy correctly.
\end{example}
\subsubsection*{Step 4: Report Extraction}\label{step_4}
To create a lightweight report for the group or individual in question, we need to extract the key information from the CDA report, e.g. redundancy US-pair, redundancy clauses, count of redundancy clauses in each part of the US (main or benefit part), and create a report as a text file with the following information:
\begin{itemize}
	
	\item A table of potential redundant pairs with the number of total redundancy clauses.
	
	\item Founded potential redundant US-pairs.
	
	\item Redundancy words and clauses of founded US-pairs. Clauses consisting of two words that have one of the relationships triggers, targets or contains.
	
	\item Text of US-pairs whose redundancy words are marked with a hash symbol (\#).
	
	\item Parts of the sentence in which words and clauses are found.
\end{itemize}
\subsubsection*{Structure of a US}
The delineation and checking of redundancy clauses within USs requires a methodical approach, especially when distinguishing between the main and the benefit part of a US-pair. This distinction is crucial to ensure that redundancy identifications within one part are not mistakenly transferred to the other. Consequently, the analytical framework comprises three conditions, each of which specifies its own methodology for case processing:
\begin{itemize}
	\item Presence of benefit in both USs of the redundancy-pair: if a benefit is identifiable in each US of the pair, a process of separation is used to split the main content from the benefit parts. After this separation, a targeted search for redundancy clauses is carried out only within the main part, whereby identified redundancies are annotated with a hash symbol (\#). This process is repeated for the benefit parts to ensure a thorough check and marking of redundancies within each individual part.
	
	\item Exclusive presence of a benefit in one US of the redundancy-pair: In scenarios where only one US of the pair contains a benefit part, the analysis is limited to the main part of both USs. The aim remains the identification and annotation of redundancy clauses within this part. The lone benefit part remains in its original state and is excluded from the redundancy check.
	
	\item Absence of benefit parts in both USs of the pair: If neither of the two USs of the redundancy-pair contains a benefit part, the focus shifts completely to the main parts. The investigation is designed to highlight redundancy clauses within these parts, whereby the benefit parts are not taken into account due to their non-existence.
	
\end{itemize}
This structured and segmented approach ensures precise and efficient identification of redundancy clauses within the USs, optimising the clarity and effectiveness of textual report.
\begin{example}
	After the CDA directory for user\_story\_12 and user\_story\_39 is created by CDA tool graphic interface (UI), we pass the location of directory into ReportExtractor class and in order to extracting the important information and save it into the texual report as well as JSON report.
	Listing \ref{list:textual_report_sample} illustrates the example of the textual report. In this case, the report only contains one US-pair.
	\begin{MyListing}
		\paragraph{}
		\centering
		%\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/TextualReportSample.txt}
		\includegraphics[scale=0.7]{Listing/TextualReportSample.png}
		\caption{Example of generated textual report for one US-pair}\label{list:textual_report_sample}
	\end{MyListing}	
	As we can see, the text report consists of a 2 x 2 table whose first column and first row are US identifiers, and the numbers inside the table are the total number (benefit part + main part) of redundancy elements between two USs; secondly, the redundant clauses related to the redundant US-pair are listed; thirdly, the text of US whose redundant phrases are marked with a hash symbol; finally, the part of the clauses in which redundant elements occur is displayed.	
\end{example}
For further evaluation purposes and easy export of the report to another platform such as Excel, a JSON report is created that collects the information about redundant US-pairs separately in a JSON object with the following entries:
\begin{itemize}
	\item Potential Redundant User Stories:  which has stored the US-pair identifier(e.g. "user\_story\_12\_AND\_user\_story\_39"). 
	
	\item Status: consisting of "Main/Beneift Part Redundancy Clauses" and "Total Redundancy Clauses", which store the count of redundancy clauses in the main and benefit part as well as in the total part of the US.
	
	\item Entity: which can consist of a "Secondary/Primary Entity" and stores the founded redundant entities.
	
	\item Common Targets/Contains: which consists of the "Main Part" and "Benefit Part" entries and only stores the targets/contains relationships that are common between the USs in a particular part of the USs. For example, if there are common redundant targets in the main part of the USs, these are included in the "Main Part" entity of the "Common Targets".
	
	\item Text: consisting of two entries, namely "First UserStory" and "Second UserStory", in which the text of the US-pair whose hash symbol has already been applied in redundant phrases is stored.
	
	\item Project Number: stores the number of the Project(e.g. "G03").
	
	\item Part of Sentence: consists of the entries "First UserStory" and "Second UserStory", in which the part of the US sentences containing redundant clauses is stored.
	
	\item All Targets/Contains: which consists of the "Main Part" and "Benefit Part" entries and stores the whole targets/contains relationships that are occurred in the particular part of the USs.
	
\end{itemize}
\begin{example}
	Listing \ref{list:json_report_sample} illustrates the example of the JSON report regarding user\_story\_12 and user\_story\_39.
\end{example}
\begin{MyListing}
	%\paragraph{}
	
	\centering
	%\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/TextualReportSample.txt}
	\includegraphics[scale=0.7]{Listing/JSONReportSample.png}
	\caption{Example of generated JSON report for one US-pair}\label{list:json_report_sample}
	
\end{MyListing}	
\subsubsection*{Step 5: Report Evaluation}
The Evaluation class, part of the \textit{org.henshin.backlog.code.evaluation} package, was developed to determine the level of redundancy in USs based on JSON reports. This class provides methods to evaluate whether two USs are either fully or partially redundant, analysing different application components of these USs.

The evaluation process involves a complex logic to determine whether USs are redundant. This includes:
\begin{itemize}
	\item Checking whether the arrays are empty or contain similar elements.
	\item Comparing the individual elements in the arrays for both USs to determine if they fully match (full redundancy) or if they have some common elements (partial redundancy).
\end{itemize}
\begin{example}
	For the two US-pairs of dataset G03, we apply the evaluation class to determine whether there is redundancy in the main or benefit part, and if so, what type of redundancy is recognised(full or partially).\\\\
	As shown in Listing \ref{list:json_evaluation}, four entries are added to the JSON report, namely "Main Partially Redundant", "Benefit Part Fully Redundant",
	"Main Part Fully Redundant", "Benefit Partially Redundant" as "Status" which their value is whether true or false. 
	\begin{MyListing}
		\paragraph{}
		
		\centering
		%\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/TextualReportSample.txt}
		\includegraphics[scale=0.7]{Listing/json_evaluation.png}
		\caption{Example of generated entries in JSON report regarding evaluation of level of redundancy in main or benefit part}\label{list:json_evaluation}
		
	\end{MyListing}	
	
	In the case of user\_story\_12 and user\_story\_39, the entry "Benefit Partially Redundant" was marked as \textit{true}, which means that US-pair in benefit parts are partially redundant.
	
\end{example}
The class performs these checks by iterating through the JSON arrays of Triggers, Targets and Contains and comparing each element with those in the common sections to determine redundancy.


\input{Section/Redundancy_Implementation}