\section{Analysing Redundancy}\label{redundancy}
% some explenation here about what will be explain here an why
%(Analyse von Redundanz als title und dann im Text Schreiben, dass es ein syntaktische Analyse ist
USs are a fundamental component of agile software development. They provide concise and clear descriptions of the software functionality from the end user's perspective. These stories guide the development process and ensure that the software meets the actual needs of the users. 

However, as the project backlog grows, they often become overloaded with redundant USs that can obscure project priorities, waste resources and complicate maintenance. Analysing the redundancy within these USs is key to maintaining clarity and efficiency in the development process.

The main goal of this analysis is to streamline the software development workflow by identifying redundancies in the USs within a project's backlog syntactically.

 By identifying and consolidating overlapping functionalities, companies can avoid redundant work, streamline development work and optimise testing processes. This strategic approach not only shortens project timelines but also improves the clarity and coherence of project documentation, making it easier for team members to navigate and manage project requirements.

In section \ref{redundancy_requirement} we present the requirements and functional needs that are used as input for the design phase to fulfil the requirements. In section \ref{desing} we explain the design decisions of the workflow shown in Figure \ref{fig:operational_flow} and explain how the architecture is structured. 

In section \ref{redundancy_implementation} follows the implementation to show how the components and their classes look in relation to Figure \ref{fig:technical_implementation}. In Section \ref{redundancy_test} we show what and how far we tested it. Finally, we apply our approach to 19 datasets of backlogs annotated with the Doccano tool \footnote {https://github.com/ace-design/nlp-stories}. In Section \ref{redundancy_evaluation} we analyse the results and in Section \ref{redundancy_conclustion} we draw a conclusion. 
\subsection{Requirements}\label{redundancy_requirement}
In order to accomplish the analysis of redundancy in USs we try to address following functional requirements:
\begin{itemize}

\item As a user, I want to perform syntactic analysis on user stories within a specified project backlog, so that I can identify and address redundancies effectively.

\item As a User, I want a report of user story pairs that contain identical syntactic clauses in both the main and benefit parts, so that I can modify them as needed.

\item As a User, I want to apply a filter to the redundancy report to exclusively display US-pairs, which have at least one clauses as \textit{Targets} with the terms "Action" (as a verb) and "Entity" (as a noun), so that I can efficiently reduce the number of potential redundant pairs.

\item As a user, I want to mark found redundancy clauses as Triggers with a hash symbol (\#) and show those that have a redundancy in \enquote{Persona} (as a noun) and \enquote{Action} (as a verb) entries, so that I can better see if the persona in is also recognised as a redundancy.

\item As a user, I want to mark founded redundancy clauses as \textit{Contains} with a hash symbol (\#) and show those that contain two \enquote{Entity}(as a noun), so that I can better see whether the contained entity is also recognised as a redundancy.

\item As a user, I would like to have a redundancy report that shows founded US texts in US-pairs and adds a hash symbol (\#) at the beginning and end of the founded words as a marker, so that I can better see which words are redundant in US-pairs.

\item As a user, I want to see how many redundancy clauses were founded in main and benefit parts of each US-pair, so that I can aggregate each founded redundancy US-pair for further statistical purposes on that basis.

\item As a user, I want a table at the top of the redundancy report that lists the US-pairs and the number of redundancy clauses contained in each pair, so that I can quickly see all the US-pairs founded and the number of redundancy clauses.

\item As a user, I want to know whether the redundancy clauses belong to the main or benefit part of the US, so that I can use it for further statistical purposes.

\item As a user, I want to know if a pair of US is partially or fully redundant, so that I can delete or modify them.
\end{itemize}
To judge the operation of a system, we define following non-functional requirements:
\begin{itemize}
	\item Testability: The system should support automated test procedures to ensure that syntactic analysis and redundancy detection work correctly. It should include comprehensive test cases covering different scenarios, including edge cases, to verify the accuracy and reliability of redundancy detection.
	
	\item Documentation: The system should include detailed documentation covering all aspects of functionality, setup. User manuals, API documentation and troubleshooting guides should be provided. 
	
	\item Performance: The system should perform the redundancy analysis within a reasonable time frame, even with large project backlogs. It should be optimised to process large amounts of data without significant performance degradation.
	
	\item Scalability: The system should be scalable to handle an increasing the number of USs and larger project backlogs.
	
	\item The system should be scalable to handle an increasing the number of USs and larger project backlogs.
	

\end{itemize}
% here should explain what was the reiquirement and why
\subsection{Design}\label{desing}
% what are the design decisions of this workflow that I had at the beginning, then describe what happens step by step, what translation I made, what is the main design decision you made for this translation, what question and that is part of the design
%In Desing should be how the architecture is based on which components and what is my own component, what does it use, what does the component look like, what class is there, is the class design, so everything comes from the Desing chapter
%In this section, the workflow process shown in Figure \ref{fig:design_phases} is described step by step. It also explains which design decisions were made for this workflow and how the architecture is based on which components.

%To fulfil the requirements mentioned in \ref{redundancy_requirement}, we use the CRF tool \cite{mosser2022modelling} for the annotation of backlogs and the Henshin API \cite{arendt2010henshin} under Java programming language for the automatic generation of rules for each US in the backlog. Moreover, we use Henshin's CDA(conflict and dependency analysis) feature \cite{mens2007analysing} to automatically recognise redundancy US-pairs.\\\\
This section describes the operational flow and architectural considerations that underpin the framework.
\subsubsection*{Design Overview}
%To meet the redundancy detection requirements specified in \ref{redundancy_requirement}, the system uses backlogs \cite{mosser2022modelling} annotated with CRF as the first artefact in workflow is the main input in our system. At the same time, the Henshin API \cite{arendt2010henshin}, which is used in a Java environment, enables the automatic generation of transformation rules for each US in the backlog. The use of Henshin's conflict and dependency analysis tool (CDA) \cite{mens2007analysing} is central to the automated identification of potentially redundant US-pairs, which is a cornerstone of framework design.
To address the redundancy detection requirements specified in Section \ref{redundancy_requirement}, our system initiates with backlogs annotated with Doccano tool\footnote{https://github.com/ace-design/nlp-stories} generated by Mosser et al. as the primary input\cite{arulmohan2023extracting}. These annotated USs are used to generate graph transformation rules for each US in the backlog using Henshin tool.

Subsequently, these rules serve as inputs for the Henshin Conflict and Dependency Analysis (CDA) tool \cite{mens2007analysing}, which automates the identification of potentially redundant US pairs. The output from the CDA tool is then utilized to create a report that compiles information on these potentially redundant pairs. This report, in turn, becomes the input for the evaluation process, which outputs statistical data concerning redundancy among USs.

Figure \ref{fig:operational_flow} illustrates how each step in this sequence is interconnected, with the output of one step feeding directly into the next. This diagram effectively demonstrates the toolchain and process workflow, highlighting how each tool transforms artefacts and contributes to the overall objective of redundancy detection.
\begin{figure}[h]
	\centering 
	\includegraphics[scale=0.4]{operational_flow}
	\caption{Step-by-step visualisation of the tool chain and its inputs and outputs}\label{fig:operational_flow}
\end{figure}
%The system ingests a graph-based model generated with the CRF tool containing a refined and annotated backlog dataset that is critical for the recognition of \textit{entities}, \textit{actions}, \textit{personas} and \textit{benefits} within the USs \cite{mosser2022modelling}. The association of each \textit{persona} with a \textit{primary action} via \textit{triggers} relationships and the association of \textit{primary/secondary actions} with \textit{primary/secondary entities} as \textit{targets} relationships, as explained by Mooser et al., form the basis for the interpretive system\cite{arulmohan2023extracting}. The output, an annotated backlog dataset as a structured JSON file, separates the annotated USs by backlog to facilitate further processing.
%As shown in Figure \ref{fig:operational_flow}, a thorough step-by-step representation of the workflow performed with different tools is provided, showing that each step has an input and an output and the output of the first step is used as input for the next step, creating a tool chain.
%along with a rationale for key design decisions and an explanation.
\paragraph{US labelling in JSON files}\label{workflow_nummerize_us}
As the annotated USs in the JSON files do not contain identifiers, a customised Python script, \textit{nummerise\_us.py}, is used to assign an unique identifier to each US, which is stored in a JSON object named \textit{"US\_Nr"} \footnote{https://github.com/amirrabieyannejad/USs\_Annotation/tree/main/Script/numberise\_us}. This addition improves the system's ability to distinguish and process individual USs within the analytical pipeline.

\paragraph{Creation of an Ecore meta model}\label{workflow_ecore}
The creation of an Ecore meta-model is required to generate rules in Henshin. The constructed meta-model, is based on the conceptual meta-model outlined in Figure \ref{fig:conceptual_metamodel} and ensures consistency with the JSON-structured data. This correspondence between the meta-model and the JSON representation underpins the rule generation and transformation processes of the system.
%The creation of an Ecore meta-model is required to generate rules in Henshin. The constructed meta-model, as shown in Figure \ref{fig:ecore_meta_model}, is based on the conceptual meta-model outlined in Figure \ref{fig:conceptual_metamodel} and ensures consistency with the JSON-structured data. This correspondence between the meta-model and the JSON representation underpins the rule generation and transformation processes of the system.

\paragraph{Rule creation process}\label{workflow_rule_creator}
The transition from identified USs to implementable transformation rules within Henshin is facilitated by the package \textit{org.eclipse.emf.henshin.model. compact}. The class \textit{RuleCreator} within the package \textit{org.henshin.backlog.code.rule} is important in this process as it uses different classes to instantiate transformation rules, nodes, edges and attributes. These components are annotated with actions such as \textit{Delete}, \textit{Create} or \textit{Preserve}, which are necessary  for the subsequent application of CDA tool to recognise redundant US-pairs. This methodological approach improves the system's ability to detect redundancies and thus optimises the efficiency of the residue analysis process.

\paragraph{Conflict and Dependency Analysis (CDA)} After the creation of transformation rules and the creation of the corresponding Henshin file by the \textit{RuleCreator} class, the system is able to perform a conflict and dependency analysis (CDA) to identify potential redundancy pairs.

\paragraph{Extraction of text reports} The creation of a text report is a crucial step in consolidating the results of the conflict and dependency analysis (CDA), which aims to highlight essential information such as the identification of potentially redundant US-pairs, the enumeration of redundancy clauses, the categorisation of these clauses within the main or benefit parts of the USs and a tabulation of potentially redundant pairs alongside the total number of redundancy clauses.

\paragraph{Evaluating the reports} After creating the reports, we are now able to evaluate the level of redundancy between US-pairs based on JSON reports.

The evaluation process involves a detailed examination of elements such as triggers, targets, and contains within the USs, comparing them to identify exact matches or significant overlaps that could indicate redundancy.

\subsubsection*{Software Architecture}\label{architectur}
In this section, we present the basic structures of our workflow and the discipline of creating such structures. Each structure comprises software elements, relations among them, and properties of both.
\begin{itemize}
	\item Annotated USs with Doccano Tool\footnote{https://github.com/doccano/doccano}: Mosser et al. used publicly available requirements from Dalpiaz et al.\cite{Dalpiaz2018} consisting of 22 product backlogs and 1,679 USs. The dataset is a raw archive of 22 text files, each containing one US per line. As there were no public expert-based annotations, Mosser et al. manually annotated the dataset using the Doccano tool for \textit{Named Entity Recognition}. Labels included persona, action, entity, benefit part and relations such as triggers, targets, and contains based on their domain meta-model.% The first author did the initial labelling, followed by several quality checks:	Initial calibration with the third author on 75 randomly selected stories.
	%Fortnightly validation sessions over two months.	Manual review of 330 randomly selected stories (19.6%) by the third author.
	%These processes resulted in 94\% agreement, which is considered excellent.
	%	integrated a dedicated Natural Language Processing (NLP) technique—Conditional Random Fields (CRF). CRFs represent a specialized class of Markov Random Fields, tailored for discriminative modelling in pattern recognition tasks where context plays a pivotal role\cite{arulmohan2023extracting}.
	As artefact we receive a graph-based model with JSON format, which represents the refined and annotated dataset for the recognition of \emph{entities}, \emph{actions}, \emph{personas} and \emph{benefits} of USs \cite{mosser2022modelling}.
	
	%Mooser et al. have linked each \emph{Persona} to each \emph{Primary Action} as \emph{Trigger} relationships, each \emph{Primary Actions} to each \emph{Primary Entity} as \emph{Target} relationships and each \emph{Primary/Secondary Entity} to each \emph{Primary/Secondary Entity} implying a \emph{Contains} relationship\cite{arulmohan2023extracting}.
	
	\item Eclipse as IDE\footnote{https://eclipseide.org/}: Eclipse is an integrated development environment (IDE) used in computer programming. It contains a base work workspace and an extensible plug-in system for customizing the environment.
	We chose this IDE because it offers the Henshin tool specifically for model-based development.
	
	\item Eclipse Modelling Project\footnote{https://eclipse.dev/modeling/}: It focuses on the evolution and promotion of model-based development technologies within the Eclipse community by providing a unified set of modelling frameworks, tooling, and standards implementations.
	
	\item Eclipse Modelling Framework (EMF)\footnote{https://eclipse.dev/modeling/emf/}: The EMF project is a modeling framework and code generation facility for building tools and other applications based on a structured data model. From a model specification described in XMI, EMF provides tools and runtime support to produce a set of Java classes for the model, along with a set of adapter classes that enable viewing and command-based editing of the model, and a basic editor.
	
	\item Henshin\footnote{https://wiki.eclipse.org/Henshin}: Henshin is an in-place model transformation language for the Eclipse Modelling Framework (EMF). It supports direct transformations of EMF model instances (endogenous transformations), as well as generating instances of a target language from given instances of a source language (exogenous transformations).
	
	Because Henshin enables the specification of restrictions and conditions within rules, we use it to enforce and verify US requirements to ensure that the restrictions are met.
		
	\item Henshin API: It provides the specification and execution of transformation modules, units and rules. This API is beneficial due to the dynamic creation and modification of transformation modules as part of an automated tool chain\footnote{https://wiki.eclipse.org/Henshin/Compact\_API}.
	
	\item RuleCreator Class: This class developed within the \textit{org.henshin.backlog.code.rule} package, serves as an integral component of our software architecture, leveraging the Henshin API to automate the process of transformation rule creation based on dynamic JSON input. This class facilitates the programmable generation of graph transformation rules. It operates by reading JSON files, which specify USs and their associated actions, entities, and relationships, then systematically constructs corresponding Henshin modules, rules, nodes, and their attributes.
	
	the RuleCreator class effectively bridges the gap between high-level requirements specified in JSON and the low-level execution capabilities of the Henshin, ensuring that USs translated into executable transformation rules that reflect the specified software behaviour. This translation process is crucial for maintaining consistency and traceability across the software development lifecycle, from requirements specification to implementation.
	
	\item Henshin's CDA Feature\footnote{https://wiki.eclipse.org/Henshin/Conflict\_and\_Dependency\_Analysis}: Henshin's conflict and dependency analysis (CDA) feature enables the detection of potential conflicts and dependencies of a set of rules.
	
	After the rules and the corresponding Henshin file have been created by the RuleCreator class as artefact, we are now able to pass them to the conflict and dependency analysis (CDA) to find potential redundancy pairs.
	
	Since the analysis of conflicts and dependencies related to the \textit{attribute} is not yet considered in the CDA API, we decided to use the user interface (UI) of the CDA extension, which supports analysis of conflict and dependencies of rules through the interactive use of CDA.
	
	\item ReportExtractor Class: The class from the \textit{org.henshin.backlog.code.report} package is used in our software architecture to extract and format reports from the CDA-generated \textit{Minimal-Model.ECore} that contain all information about redundant US-pairs such as redundant elements with their name and type. It uses classes from the \textit{org.eclipse.emf.ecore} package to handle EMF (Eclipse Modelling Framework) resources that support the management and manipulation of \textit{Minimal-Model.ECore} data in a structured format.
	
	In operation, the ReportExtractor class reads minimal-model.ecore files containing detailed information about redundant US-pairs. It then processes this data to generate reports in both text and JSON formats, aiding in the systematic analysis of potential redundancies within the USs. This is facilitated through methods that dynamically read and interpret the JSON data, extracting key information such as actions, entities, and their interactions.
	
	\item Evaluation Class: The class is part of the \textit{org.henshin.backlog.code.evaluation} package, serves a critical function within our software architecture, focusing on the analysis and determination of redundancy between USs based on specified criteria. This class utilizes JSON processing to assess the overlap and redundancy between elements of USs, such as triggers, targets, and contains, which are vital for identifying potential redundancies in the USs.
	
	Key functionalities of this class include reading and interpreting JSON data, where it extracts detailed elements related to USs and evaluates them against redundancy criteria defined in its methods. The class's method \textit{evaluateRedundancyCriteria} is particularly central, as it processes the JSON objects representing USs to determine if they are fully or partially redundant based on the presence and matching of various JSON array elements like triggers, targets, and contains in the main and benefit parts of the USs.
	
	\end{itemize}
Figure \ref{fig:technical_implementation} shows the architectural composition, highlighting the integral components and their user interface and artefacts.
\begin{figure}[h]
	\centering 
	\includegraphics[scale=0.6]{technical_implementation}
	\caption{Design phases}\label{fig:technical_implementation}
\end{figure}
%\subsubsection*{CRF Tool}\label{workflow_crf}
%As input, we receive a graph-based model generated by the CRF tool, which represents the refined and annotated dataset for the recognition of \emph{entities}, \emph{actions}, \emph{persons} and \emph{benefits} of USs \cite{mosser2022modelling}.
%Mooser et al. have linked each \emph{Persona} to each \emph{Primary Action} as \emph{Trigger} relationships, each \emph{Primary Actions} to each \emph{Primary Entity} as \emph{Target} relationships and each \emph{Primary/Secondary Entity} to each \emph{Primary/Secondary Entity} implying a \emph{Contains} relationship\cite{arulmohan2023extracting}. As output we receive a JSON-file which contains all annotated USs separated by each backlogs.
%and its level as \textit{full} or \textit{partial} redundancy in terms of clauses

Regarding redundancy, some definitions are clarified:
\begin{definition}[\textbf{Main and Benefit Parts in User Story}]
	A user story (US) is divided into two distinct parts that collectively describe what the user wants, why they want it, and how it will benefit them. These are:
	\begin{itemize}
		\item The \textit{main part}, which is essential as it clearly and concisely summarises the persona, the intended functionality and the resources required to perform the action. This part usually has follow the format: \textit{"As a [persona], I want [what]"}. 
		
		Here, the persona helps contextualise the requirement by linking it to a user type, which promotes understanding and empathy. The intended functionality describes the action that the persona wants to perform or the function they need, providing a clear statement of the requirement.
		
		Specifying the resources required to perform the action helps with planning and resource allocation and ensures that the development team is aware of the tools, technologies and time required.
		
		\item The \textit{benefit part}, which formulates the potential benefit for the end user and typically begins with the phrase \enquote{so that}. This part of the US is used for justifying the need for a feature by explaining the value or improvement it brings to the user's experience. It connects the functionality directly to user satisfaction, efficiency, or productivity gains, making it easier for the development team to prioritize features based on their impact.
		
		It is worth noting that sometimes the benefit part may not exist in the structure of USs. In such cases, the main part can stand alone.
	\end{itemize}
\end{definition}
\begin{definition}[\textbf{Clause}]
	In this context, a clause is constructed as a pair of words, each pair being linked by a set of predefined relational constructs. These constructs, which are central to the contextual structuring of USs, are described below:
	\begin{itemize}
		
		\item Triggers: A relationship between a persona and an action that includes the invocation of the action by the persona within the narrative of a US.
		
		\item Targets: A relationship between an action and an entity means that the action influences the entity in question.
		
		\item Contains: A relation between two entities, indicating that one entity contains another one, in which one entity merges into or is enclosed by another. 
		
	\end{itemize}
	Including the specified clauses in the analysis framework ensures that the words identified in potentially redundant US-pairs are linked by one of the relational constructs outlined: \textit{Triggers}, \textit{Targets}, or \textit{Contains}. 
	
	This methodological approach supports the identification of redundancy not only on the basis of lexical similarity, but through the contextual relationships that determine the interactions within the US narratives.
\end{definition}
\begin{example}
	When evaluating the textual similarities between two USs, consider the following pair: \\
	user\_story\_01: \textit{"As a Staff member, I want to Manage Ordinances, so that I can maintain accurate ordinance information in the System."}\\
	user\_story\_02: \textit{"As a Staff member, I want to Manage Affidavits, so that I can ensure compliance with the requirements prior to the hearing."} \\\\
	Upon comparing these two user stories, we identify shared phrases: "Staff member" and "Manage." These common elements indicate that both user stories involve the same persona ("Staff member") engaged in a similar type of action ("Manage"). However, the focus of their management tasks differs significantly, with one managing "Ordinances" and the other "Affidavits."\\\\
	This distinction in the resources being managed suggests that while there are textual overlaps and commonalities in persona and action, the user stories are not duplicates, as they address different aspects of the staff member's responsibilities and goals within the organization.
\end{example}
\begin{definition}[\textbf{Redundancy}]
	Redundancy refers to situations where all phrases of USs duplicate or largely overlap with others. This is the case when only a part (main or benefit) or entire USs are syntactically identical, meaning that some or all phrases and their orderings match perfectly between two USs. \\\\
	$Notation$. Lowercase identifiers refer to single elements, and uppercase identifiers denote sets. A user story $us$ is a 2-tuple $us = \langle m,b\rangle $ where:
	\begin{itemize}
		\item A main $m$ is a 6-tuple $m = \langle p,A,E,Tr,Ta,Co\rangle $ which $p$ is the persona, $A = \{ a_1,a_2,...\} $ is a set of actions, $E = \{e_1,e_2,...\}$ is a set of entities, trigger references $Tr = \{(p_1,a_1),(p_2,a_2),...\}$ is a set of pairs of persona and action, target references $Ta = \{(a_1,e_1),(a_2,e_2),...\}$ is a set of pairs of action and entity, and contain references $Co = \{ (e_1,e_2),(e_*,e_*),... \}$ is a set of pairs of entities.
		
		\item A benefit $b$ is a 4-tuple $b = \langle A,E,Ta,Co\rangle $ where $A = \{ a_1,a_2,...\} $ is a set of actions, $E = \{e_1,e_2,...\}$ is a set of entities,  $Ta = \{(a_1,e_1),(a_2,e_2),...\}$ is a set of pairs of action and entity, $Co = \{ (e_1,e_2),(e_*,e_*),... \}$ is a set of pairs of entities.
	\end{itemize}
	To denote that a syntactic operator, we add the subscript
	“syn”; for instance, $=_{syn}$ is syntactic equivalence which introduced by Lucassen et al. \cite{lucassen2016improving}.\\ In the following, let $us_1 = \langle m_1,b_1\rangle $ which $m_1 = \langle p_1,a_1,e_1,tr_1,ta_1,co_1 \rangle$ and $b_1 = \langle a_1,e_1,ta_1,co_1 \rangle$ as well as $us_2 = \langle m_2,b_2\rangle$ which $m_2 = \langle p_2,a_2,e_2,tr_2,ta_2,co_2 \rangle$ and $b_2 = \langle a_2,e_2,ta_2,co_2 \rangle$.
	
	The entity $e_1$ is an exact redundant of entity $e_2$, when the entities are identical. Formally,\\
	$isRedundant(e_1,e_2) \leftrightarrow e_1 =_{syn} e_2$\\\\
	The action $a_1$ is an exact redundant of action $a_2$, when the actions are identical. Formally,\\
	$isRedundant(a_1,a_2) \leftrightarrow a_1 =_{syn} a_2$\\\\
	The trigger reference $tr_1$ is an exact redundant of trigger $tr_2$, if they run between redundant personas $p_1$ and $p_2$ and redundant actions $a_1$ and $a_2$. Formally,\\
	$isRedundant((p_1,a_1),(p_2,a_2)) \leftrightarrow tr_1 =_{syn} tr_2$\\\\
	The target reference $ta_1$ is an exact redundant of target $ta_2$, if they run between redundant actions $a_1$ and $a_2$ and redundant entities $e_1$ and $e_2$. Formally,\\
	$isRedundant((a_1,e_1),(a_2,e_2)) \leftrightarrow ta_1 =_{syn} ta_2$\\\\
	The contain reference $co_1$ is an exact redundant of contain $co_2$, if they start at redundant entities $e_1$ and $e_2$ and end in redundant entities $e_3$ and $e_4$. Formally,\\
	$isRedundant((e_1,e_3),(e_2,e_4)) \leftrightarrow co_1 =_{syn} co_2$\\\\
	To comprehensively assess redundancy, it is important to consider not only the textual content, but also the functional and contextual relevance of each phrase within the USs. By analysing triggers, targets and contains relationships, we can uncover redundancies that may not be immediately apparent through a simple text comparison.
\end{definition}	
\begin{definition}[\textbf{Full Redundancy}]
	A US-pair are fully redundant in their main parts if
	\begin{itemize}
		\item there are redundant target references $Ta_2$ in $m_2$ for all target references $Ta_1$ in $m_1$, so that $Ta_1$ and $Ta_2$ are redundant, and vice versa, and
		
		\item there are redundant trigger references $Tr_2$ in $m_2$ for all trigger references $Tr_1$ in $m_1$, so that $Tr_1$ and $Tr_2$ are redundant, and vice versa, and
		
		\item there are redundant contain references $Co_2$ in $m_2$ for all contain references $Co_1$ in $m_1$, so that $Co_1$ and $Co_2$ are redundant, and vice versa.
		
	\end{itemize}
	A US-pair are fully redundant in their benefit parts if
	\begin{itemize}
		\item there are redundant target references $Ta_2$ in $b_2$ for all target references $Ta_1$ in $b_1$, so that $Ta_1$ and $Ta_2$ are redundant, and vice versa, and
		
		\item there are redundant contain references $Co_2$ in $b_2$ for all contain references $Co_1$ in $b_1$, so that $Co_1$ and $Co_2$ are redundant, and vice versa.
		
	\end{itemize}
	A US-pair was categorised as "full redundancy" in the main/benefit part if all occurring clauses in the main/benefit part consisting of triggers (for the main part only), targets and contains were syntactically identical.\\\\
	This means that in each pair, the wording, order and structure of the clauses relating to the triggers and targets must be identical to fall into this category. The check also extends to the "contain" elements, if these are present, to ensure that these also match perfectly and without deviations.\\\\
	Full redundancy in the main part means that the redundant stories do not provide additional information and can be consolidated or eliminated without compromising the completeness or operational integrity of the system specifications.\\\\
	On the other hand, having this level of redundancy in the benefit part means that USs achieve the same goal; they can be categorised in the same group. This means that if the end results or benefits described by the USs are identical, regardless of the different actions/entities or triggers that lead to these results, the USs effectively serve the same purpose within the system.
\end{definition}
\begin{example}
	For example, the following US-pair is identified as \enquote{Full Redundancy} in the main part:
	
	\textit{user\_story\_01:} \enquote{\#g14\# as a \#publisher\#, i want to \#publish\# a \#dataset\#, so that i can view just the dataset with a few people.}
	
	\textit{user\_story\_02:} \enquote{\#g14\# as a \#publisher\#, i want to \#publish\# a \#dataset\#, so that i can share the dataset publicly with everyone.}\\\\
	All existing clauses associated with the main elements in this scenario - in particular the trigger (where "Publisher" is the persona and "Publish" is the action) and the targets (where "Publish" is the action and "Dataset" is the entity) - are exactly the same in the main parts of the USs. This complete similarity shows that there is "full redundancy" in the main parts of these USs.
\end{example}
\begin{example}
	Following US-pair is identified as \enquote{Full Redundancy} in \enquote{Benefit} part:
	
	\textit{user\_story\_02:} \#g05\# as a data publishing user, I want to be able to edit the model of data I have already imported, so that I can \#fix\# \#bugs\# or \#make\# \#enhancements\# in the \#API\# built for my \#data\#.
	
	\textit{user\_story\_07:} \#g05\# as a data publishing user, I want to be able to edit the data source of data I have already imported, so that i can \#fix\# \#bugs\# or \#make\# \#enhancements\# in the \#API\# built for my \#data\#.
	
	As we can see, targets(["fix", "bugs"], ["make", enhancements"]) and contains (["API", "enhancements"], ["data","API"]) in the benefit part are identical between USs, therefore, we have \enquote{Full Redundancy} in the \enquote{Benefit} parts.
\end{example}
Last but not least, sometime there are words in the annotated USs that are not labelled as a reference (trigger, target or contain) by the Doccano tool at all, which leads to a full redundancy being incorrectly recognised instead of a partial.
\begin{example}
	Considering following US-pair:
	
 	user\_story\_26: \#g05\# as an \#api user\#, i want to be able to \#understand\# if a \#user\# is an administrator, so that [...].
 	
 	user\_story\_25: \#g05\# as an \#api user\#, i want to be able to \#understand\# if a \#user\# is a publisher, so that [...].\\\\
	In this example, full redundancy is found due to the redundant targets and triggers in the main part, which is not entirely correct. This is because there are some phrases that do not appear as contain or target in any reference, such as: "administrator" (user\_story\_26) and "publisher" (user\_story\_25).
\end{example}
\begin{definition}[\textbf{Partial Redundancy}]
	A US-pair are partially redundant in their main parts if there is a target reference $ta_1$ in $m_1$ and a target reference $ta_2$ in $m_2$, so that $ta_1$ and $ta_2$ are redundant.\\\\
	A US-pair are partially redundant in their benefit parts if there is a target reference $ta_1$ in $b_1$ and a target reference $ta_2$ in $b_2$, so that $ta_1$ and $ta_2$ are redundant.\\\\
	Note that in this definition, a US-pair that is fully redundant in the main part (or in the benefit part) is also partially redundant in the main part (or in the benefit part).\\\\
	Partial redundancy in USs occurs when only certain clauses, such as targets, have significant overlap but are not completely identical. This means that while there are shared aspects such as targets between USs, other clauses such as triggers or contains may not overlap, indicating an incomplete match. Such a scenario indicates that there is substantial, but not fully, a match between the USs.
	%	Was found when only some clauses were redundant in either the Main or Benefit parts, while others were unique. This indicates overlapping clauses, but not full redundancy.
\end{definition}
\begin{example}
	Following US-pair is identified as \enquote{Partial Redundancy} in main part:
	
	\textit{user\_story\_09:} \#g04\# as a \#user\#, I want to be able to \#view\# a \#map display\# of the public recycling bins around my \#area\#.
	
	\textit{user\_story\_10:} \#g04\# as a \#user\#, I want to be able to \#view\# a \#map display\# of the special waste drop off sites around my \#area\#.
	
	As we can see, there are some redundancy clauses such as Triggers (["user", "view"]), Targets (["view", "map display"]) and Contains(["map display", "area"]) between the USs. There are also clauses such as Contains (["public recycling bins"] vs. ["hazardous waste collection points"]) that are distinct elements justifying the maintaining of separate USs. We therefore assess this as "Partial redundancy" in the "Main part".
\end{example}
\begin{example}
	Following US-pair is identified as \enquote{Partial Redundancy} in benefit part:
	
	\textit{user\_story\_17:} \#g03\# as a staff member, I want to manage approved proffers, so that I can \#ensure\# \#compliance\# with and satisfaction of the proffer in the future.
	
	\textit{user\_story\_30:} \#g03\# as a staff member, I want to manage affidavits, so that I can \#ensure\# \#compliance\# with the requirements prior to the hearing.
	
	As we can see, there is a redundancy clause as targets (["ensure", "compliance"]) between the USs in the benefit part, but there is also a clause as contains (["proffer", "satisfaction"] vs ["requirements", "hearing"]), which leads us to evaluate this as \enquote{Partial Redundancy} in the \enquote{Benefit} part.
\end{example}
\subsubsection*{Design Phases}\label{design_phases}
To provide a comprehensive overview of the design phases, this section explains each step of the process, from initial setup to final evaluation, using practical examples.
\subsubsection*{Step 1: Data Preparation}\label{design_step_1}
As primary input, we receive a graph-based model generated by the Doccano tool, which represents the refined and annotated dataset for the recognition of \emph{entities}, \emph{actions}, \emph{persons} and \emph{benefits} of USs \cite{arulmohan2023extracting}.

The datasets have the JSON format, the structure of which is very important in the Java classes \textit{RuleCreator}, \textit{ReportExtractor}, and \textit{Evaluation}. Therefore, understanding the JSON format provided is needed for the further procedure.

Each JSON file for a backlog dataset contains a JSON-array in which each US entry is defined as a JSON-object. Listing \ref{list:desing_json_format} illustrates the format used for the US entry.
\begin{MyListing}
	\paragraph{}
	\hrule
	\centering
	\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/json_format.json}
	\caption{The JSON format of each US entry in JSON file}\label{list:desing_json_format}
	\hrule
\end{MyListing}
Mosser et al. have linked each \emph{Persona} to each \emph{Primary Action} as \emph{Trigger} relationships, each \emph{Primary Actions} to each \emph{Primary Entity} as \emph{Target} relationships and each \emph{Primary/Secondary Entity} to each \emph{Primary/Secondary Entity} implying a \emph{Contains} relationship\cite{arulmohan2023extracting}.

To interact with the entries in JSON file, we need to distinguish between the entries that are defined as JSON-objects, such as: {Text, Action, Entity, Benefit} and the entries that are defined as a JSON-array, such as: {Persona, Primary/Secondary Action, Primary/Secondary Entity, Triggers, Targets, Contains}.
\subsubsection*{Identifying USs in JSON-File}\label{desing_workflow_nummerize_us}
Annotated USs in each JSON file have no identifier. To distinguish USs, we use a Python script called \textit{nummerise\_us.py} \footnote{https://github.com/amirrabieyannejad/USs\_Annotation/tree/main/Script/numberise\_us}, which receives JSON files as input and adds a JSON object named \enquote{US\_Nr} with an identifier as value (e.g. user\_story\_01) to each US and returns the JSON files as output.
Listing \ref{list:desing_json_format_sample} illustrates the added JSON object "US\_Nr" and its value in the JSON file.
\begin{MyListing}
	\paragraph{}
	\hrule
	\centering
	\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/json_format_sample.json}
	\caption{The JSON format with the additional JSON object "US\_Nr" and its value}\label{list:desing_json_format_sample}
	\hrule
\end{MyListing}
\subsubsection*{Step 2: Creation of Rules}\label{design_step_2}
Step 2 of the design involves a central process in which the US data structured in JSON files is transformed into transformation rules using the Henshin API. This involves the creation of an Ecore meta-model that represents the structure of the data we are working with, followed by the generation of Henshin transformation rules using RuleCreator class.
\subsubsection*{Creating Ecore Meta-Model}\label{design_workflow_ecore}
To be able to create rules in Henshin, an Ecore (meta)-model should be available. Ecore is the core (meta)-model at the heart of the EMF (Eclipse Modelling Framework). It enables the formulation of other models by utilising its constructs.

Accordingly, we create an Ecore meta-model as shown in Figure \ref{fig:design_ecore_meta_model}, which is inspired by the meta-model shown in Figure \ref{fig:conceptual_metamodel} and corresponds to the JSON-objects in the JSON-file as follows:
\begin{itemize}
	\item \textit{Persona} as a class in the meta-model corresponds to the JSON-object \enquote{Persona} in the JSON-file.
	\item \textit{Entity} as an abstract class, from which \textit{Primary/Secondary Entity} inherits as a class in the meta-model, corresponds to the JSON-object \enquote{Entity}, which contains two JSON-arrays, namely \enquote{Secondary/Primary Entity} in the JSON-file.
	\item \textit{Action} as an abstract class and \textit{Primary/Secondary Action} as an inherited class in the meta-model correspond to the JSON-object \enquote{Action}, which contains two JSON-arrays, namely \enquote{Secondary/Primary Action} in the JSON-file.
	\item \textit{Benefit} as a class in the meta-model, which also has an attribute called \enquote{text} that corresponds to the JSON-object \enquote{Benefit} in the JSON-file.
	\item \textit{Story} as a class in the meta-model that contains text from US, which also has an attribute called \enquote{text} that corresponds to the JSON-object \enquote{Text} in the JSON-file.
	\item Abstract class \textit{NamedElement} has attribute \textit{name}, which Primary/Secondary Action/Entity inherit from it, which corresponds to the value of \textit{Primary/Secondary Action/Entity} in JSON-file.
	\item \textit{Edge} with the name \textit{triggers} between Persona and Primary Action in the meta-model, which corresponds to the JSON-array \enquote{Triggers}, where each JSON-array in it contains a pair, the first element corresponding to the \textit{Persona} and the second to the \textit{Primary Action}.
	\item \textit{Edge} named \textit{targets} between Primary/Secondary Action and Primary/Secondary Entity in the meta-model, which corresponds to the JSON-array \enquote{Targets}, where each JSON-array has a pair, the first element corresponding to \enquote{Primary/Secondary Action} and the second element corresponding to \enquote{Primary/Secondary Entity}.
	\item \textit{Edge} named \textit{contains} between Primary/Secondary Entity and itself in the meta-model, which corresponds to the JSON-array \enquote{Contains}, where each JSON-array in it has a pair where the first element corresponds to \enquote{Primary/Secondary Entity} and the second element corresponds to \enquote{Primary/Secondary Entity}.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{ecore_metamodel}
	\caption{Ecore meta-model inspired by Mosser et al. \cite{mosser2022modelling}}\label{fig:design_ecore_meta_model}
\end{figure}
\subsubsection*{Creating Rules}\label{design_workflow_rule_creator}
With the identified USs in the the JSON-file, we generate rules with the Henshin package \textit{org.eclipse.emf.henshin.model.compact}, which is responsible for the creation of \textit{transformation rules} and their \textit{classes}, \textit{attributes}, \textit{edges} and annotates them with \textless\emph{Delete}\textgreater, \textless\textit{Create}\textgreater or\textless\textit{Preserve}\textgreater, which are vital for the CDA tool to recognise the redundant pairs.

To generating rules we create a package named \textit{org.henshin.backlog.code.rule} and specially the class \textit{RuleCreator} which used following classes\footnote{https://wiki.eclipse.org/Henshin/Compact\_API}:
\begin{itemize}
	\item \textit{org.eclipse.emf.henshin.model.compact.CModule}: CModule class can import elements from an Ecore file to use them in the transformation process responsible for linking the Ecore meta-model to the Henshin-file to be created.
	\item \textit{org.eclipse.emf.henshin.model.compact.CRule}: Once we have a CModule, we can specify transformation rules with the CRule class and create them.
	\item \textit{org.eclipse.emf.henshin.model.compact.CNode}: Now that we have a transformation rule, we want to fill this rule with nodes, edges and attributes. To create a node within a transformation rule, we need the CRule class. To create an edge we need to reference two nodes together. The default action when specifying a node, edge or an attributes is the \textless\emph{preserve}\textgreater action. We can also specify a different action when we create a node or an edge, for example \textless\emph{delete}\textgreater or\textless\emph{create}\textgreater.
	\item org.henshin.backlog.code.rule.RuleCreator: We implement \textit{RuleCreator} class that creates a rule with annotated nodes, edges and attributes based on a JSON-file as input and a Henshin-file containing all rules as output, where each rule and its members(nodes, attributes, edges) correspond to the individual US and their JSON-objects/arrays in the JSON-file. 
	
	The most important design decision of this class is the way nodes, attributes and edges are annotated in order to be able to apply conflict and dependency analysis (CDA) for rules stored as a Henshin file.
	
	We decided to annotate the \enquote{name} attribute of all Primary/Secondary Actions/Entities and their associated edges including \enquote{targets}, \enquote{triggers} and \enquote{contains} as \textless delete\textgreater  action. 
	
	The main goal is to increase the probability of identifying US-pairs characterised by matching names of \textit{action} nodes and \textit{entity} nodes in conjunction with an edge called \textit{targets}. This congruence serves as a basic criterion for identifying potentially redundant US-pairs and simplifies the process of redundancy detection in the context of US analysis.
\end{itemize}
\begin{example}
	Listing \ref{list:design_json_user_story_12} shows the JSON format in relation to user\_story\_12 and Figure \ref{fig:desing_rule_user_story_12} shows the application of the RuleCreator class in this US, which is a transformation rule where the targets and the associated contains relationships are annotated as a \textless Delete\textgreater action and the rest of the nodes and edges are annotated as a \textless Preserve\textgreater action.\\\\
	Text of US is:
	user\_story\_12: "\#G03\# As a Staff member, I want to Assign an Application for Detailed Review, so that I can review the for compliance and subsequently approved or denied."
	%Considering the backlog dataset as shown in Listing \ref{list:backlog_g03}:
	\begin{MyListing}
		\paragraph{}
		\centering
		\includegraphics[scale=0.8]{Listing/json_user_story_12.png}
		\caption{JSON entities Related to user\_story\_12}\label{list:design_json_user_story_12}
	\end{MyListing}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.6]{rule_user_story_12}
		\caption{Generated transformation rule related to user\_story\_12 using RuleCreator class}\label{fig:desing_rule_user_story_12}
	\end{figure}
	As we can see, the "targets" edges and their direct relationships ("triggers" and "contain", if any) are also annotated as \textless delete\textgreater, which is very important to find redundant elements with the CDA tool.
\end{example}
\begin{example}
	Listing \ref{list:design_json_user_story_39} shows the JSON entities related to user\_story\_39 and Figure \ref{fig:design_rule_user_story_39} shows transformation rule generated by RuleCreator class.\\\\
	Text of US is:
	user\_story\_39: "\#G03\# As a Plan Review Staff member, I want to Review Plans, so that I can review them for compliance and either approve, or fail or deny the plans and record any conditions, clearances, or corrections needed from the Applicant."
	\begin{MyListing}
		\paragraph{}
		\centering
		\includegraphics[scale=0.8]{Listing/json_user_story_39.png}
		\caption{JSON entities Related to user\_story\_39}\label{list:design_json_user_story_39}
	\end{MyListing}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.43]{rule_user_story_39}
		\caption{Generated transformation rule related to user\_story\_39 using RuleCreator class}\label{fig:design_rule_user_story_39}
	\end{figure}
	Attribute "Plan", as we can see, it appears both in the main part as a primary entity and in the benefit part as a secondary entity, forming the various relationships as targets and contains.
\end{example}
\begin{example}
	Listing \ref{list:desing_json_user_story_51} shows the JSON entities related to user\_story\_51 and Figure \ref{fig:desing_rule_user_story_51} shows transformation rule generated by RuleCreator class.\\\\
	Text of US is:
	user\_story\_51: "\#G03\# As an Enforcement Staff member, I want to Issue a Notice of Violation, so that I can provide formal communication to the responsible party."
	\begin{MyListing}
		\paragraph{}
		\centering
		\includegraphics[scale=0.8]{Listing/json_user_story_51.png}
		\caption{JSON entities Related to user\_story\_51}\label{list:desing_json_user_story_51}
	\end{MyListing}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.55]{rule_user_story_51}
		\caption{Generated transformation rule related to user\_story\_51 using RuleCreator class}\label{fig:desing_rule_user_story_51}
	\end{figure}
	Last but not least, we have determined that the secondary entity "responsible party" has neither a target nor a contains relationship. Therefore, it is not annotated as \textless delete\textgreater, but as \textless preserve\textgreater. This is due to the fact that some phrases have been identified as entities in the Doccano tool, but their relationship is not annotated at all, which is problematic for analysing redundancy.
\end{example}
\subsubsection*{Step 3: Conflict and Dependency Analysis}\label{step_3}
After the rules and the corresponding henshin file have been created by the RuleCreator class, we are now able to pass them to the conflict and dependency analysis (CDA) to find potential redundancy pairs.

Since the analysis of conflicts and dependencies related to the \textit{attribute} is not yet considered in the CDA API \footnote{https://wiki.eclipse.org/Henshin/conflict\_and\_Dependency\_Analysis}, we decided to use the user interface (UI) of the CDA extension of Henshin, which supports analysis of conflict and dependencies of rules through the interactive use of CDA.

To apply CDA to Henshin files, we just need to right-click on the Henshin file and select \textit{Henshin} -\textgreater\textit{ conflict and Dependency Analysis} from the context menu as shown in Figure \ref{fig:henshin_context_menu}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{henshin_context_menu}
	\caption{Applying CDA to the selected Henshin file}\label{fig:henshin_context_menu}
\end{figure}
A user interface then appears, prompting to select the rule sets to be analysed and the type of analysis. We then select as \enquote{\textit{First}} and \enquote{\textit{Second} \textit{Rules}}, all rules related to USs. Additionally, as the type of analysis we select \enquote{\textit{conflicts}} as illustrated in Figure \ref{fig:select_rules}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{select_rules}
	\caption{CDA user interface: Selection of rules and type of analysis}\label{fig:select_rules}
\end{figure}
On the next page of the CDA UI shown in Figure \ref{fig:select_granularity}, we specify the depth of analysis that we use with \enquote{\textit{Fine granularity}} when selecting \enquote{\textit{Create a complete result table}} and \enquote{\textit{Create an abstract result table}}. 

Fine granularity provides a detailed examination of each conflicting rule by listing all conflict reasons. Unlike coarse granularity, which focuses only on minimal conflict reasons, fine granularity includes both minimal and more general conflict reasons. The binary granularity, where simple conflicting rule pairs are listed, may be too simple for complex systems where understanding the nature of the conflict is essential for the solution. 
We choose \enquote{Fine granularity} as the depth of analysis due to the fact that it shows all conflict reasons for each conflicting rule pair. This allows for a deeper understanding of how different model fragments contribute to conflicts.

A conflict reason is a model fragment whose presence leads to a conflict. General conflict reasons result from different combinations of minimal conflict reasons\cite{cda_api}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{select_granularity}
	\caption{CDA user interface: Selection of report granularity}\label{fig:select_granularity}
\end{figure}
During the execution of the CDA analysis, the rule pairs is analysed and a conflict analysis is performed. Once the calculation is complete, the results are listed in the \enquote{CDA} -\textgreater \enquote{Result window}, as shown in Figure \ref{fig:cda_report}. The top entry shows the granularity, which in our case is \enquote{Fine}. These entries contain the rule pairs that conflict with each other. Each rule pair contains a number of conflict reasons.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{cda_report}
	\caption{CDA report with fine granularity}\label{fig:cda_report}
\end{figure}
Figure \ref{fig:cda_report_in_project_dir} shows how the data is saved in the project tree view. The results directory is created in the directory containing the Henshin that was used for the analyses. The new folder name is the date and time at which the analysis was performed. In contrast to the \enquote{\textit{CDA/Results}} view, this folder contains all conflict reasons and atoms together in a rule pair directory.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{cda_report_in_project_dir}
	\caption{Saving CDA results data in the project structure view}\label{fig:cda_report_in_project_dir}
\end{figure}
For each conflict reason, there is a \enquote{\textit{minimal-model.ecore}} file, that contains packages in which various conflict elements such as \enquote{attributes} and \enquote{references} (edges) are mapped together and displayed in different packages.

Figure \ref{fig:minimal_model_packages} shows the representation of the conflicting attributes and references. An attribute has the property of changing the value and is represented by an arrow \enquote{-\textgreater}. The attribute from the first rule is separated from the second rule by an underscore, just as with the nodes.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{minimal_model_packages}
	\caption{Representation of redundant attributes and references in \textit{minimal-model.ecore} file}\label{fig:minimal_model_packages}
\end{figure}
\begin{example}
	To illustrate this step, we also pass the transformation rules created in step 2, which reflect three USs (user\_story\_12/39/51), to the CDA tool and selecting "conflicts" as conflict type to calculate with "fine granularity" as the depth of analysis.\\
	Once the calculation is complete, the results are listed in the "CDA" -\textgreater "Results Window" as shown in Figure \ref{fig:step_3_cda_report}. Figure \ref{fig:step3_cda_report_project_tree_view} shows how the data is saved in the project's tree view, which contains all conflict reasons and atoms together in a rule pair directory.
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{step_3_cda_report}
		\caption{CDA report in relation to three transformation rules with "conflicts" as the type to be calculated and "fine granularity" as the depth of the analysis}\label{fig:step_3_cda_report}
	\end{figure}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{step3_cda_report_project_tree_view}
		\caption{Saving CDA results data in the project's tree view}\label{fig:step3_cda_report_project_tree_view}
	\end{figure}
	As we can see, the CDA tool has only found redundancy between user\_story\_12 and user\_story\_39. This is because there are no redundant clauses between two USs (user\_story\_12 and user\_story\_39) and user\_story\_51.\\
	Regarding the redundant elements specifically, we can refer to the created file "minimal-model.ecore", which is located in the tree view of the project under each conflict reason. Figure \ref{fig:step3_cda_report_project_tree_view} show the minimal-model.ecore file related to user\_story\_12 and user\_story\_39.\\\\
	The file Minimal-model.ecore, which refers to user\_story\_12 and user\_story\_39, is divided into packages, with each package containing different matches of redundant elements. If there is a redundancy between two elements, this is explicitly indicated by a hash symbol (\#). Figure \ref{fig:step3_cda_package} illustrates the redundant elements found in each package.\\\\
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{minimal_model_packages}
		\caption{Representation of the redundant elements in each package within \textit{minimal-model.ecore} file}\label{fig:step3_cda_package}
	\end{figure}
	For example, the attribute "name" with the value "review" in "Secondary Action" and the attribute "targets" with the value "compliance" in "Secondary Entity" are labelled as redundant (both with a hash symbol). The reference to "targets" is also marked as redundant (with a hash symbol), which means that the reference (targets) from "Secondary Action" to "Secondary Entity" is also redundant between USs, which is a very important criterion for finding redundancy correctly.
\end{example}
\subsubsection*{Step 4: Report Extraction}\label{step_4}
To create a lightweight report for the group or individual in question, we need to extract the key information from the CDA report, e.g. redundancy US-pair, redundancy clauses, count of redundancy clauses in each part of the US (main or benefit part), and create a report as a text file with the following information:
\begin{itemize}
	
	\item A table of potential redundant pairs with the number of total redundancy clauses.
	
	\item Founded potential redundant US-pairs.
	
	\item Redundancy words and clauses of founded US-pairs. Clauses consisting of two words that have one of the relationships triggers, targets or contains.
	
	\item Text of US-pairs whose redundancy words are marked with a hash symbol (\#).
	
	\item Parts of the sentence in which words and clauses are found.
\end{itemize}
\subsubsection*{Structure of a US}
The delineation and checking of redundancy clauses within USs requires a methodical approach, especially when distinguishing between the main and the benefit part of a US-pair. This distinction is crucial to ensure that redundancy identifications within one part are not mistakenly transferred to the other. Consequently, the analytical framework comprises three conditions, each of which specifies its own methodology for case processing:
\begin{itemize}
	\item Presence of benefit in both USs of the redundancy-pair: if a benefit is identifiable in each US of the pair, a process of separation is used to split the main content from the benefit parts. After this separation, a targeted search for redundancy clauses is carried out only within the main part, whereby identified redundancies are annotated with a hash symbol (\#). This process is repeated for the benefit parts to ensure a thorough check and marking of redundancies within each individual part.
	
	\item Exclusive presence of a benefit in one US of the redundancy-pair: In scenarios where only one US of the pair contains a benefit part, the analysis is limited to the main part of both USs. The aim remains the identification and annotation of redundancy clauses within this part. The lone benefit part remains in its original state and is excluded from the redundancy check.
	
	\item Absence of benefit parts in both USs of the pair: If neither of the two USs of the redundancy-pair contains a benefit part, the focus shifts completely to the main parts. The investigation is designed to highlight redundancy clauses within these parts, whereby the benefit parts are not taken into account due to their non-existence.
	
\end{itemize}
This structured and segmented approach ensures precise and efficient identification of redundancy clauses within the USs, optimising the clarity and effectiveness of textual report.
\begin{example}
	After the CDA directory for user\_story\_12 and user\_story\_39 is created by CDA tool graphic interface (UI), we pass the location of directory into ReportExtractor class and in order to extracting the important information and save it into the texual report as well as JSON report.
	Listing \ref{list:textual_report_sample} illustrates the example of the textual report. In this case, the report only contains one US-pair.
	\begin{MyListing}
		\paragraph{}
		\centering
		%\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/TextualReportSample.txt}
		\includegraphics[scale=0.7]{Listing/TextualReportSample.png}
		\caption{Example of generated textual report for one US-pair}\label{list:textual_report_sample}
	\end{MyListing}	
	As we can see, the text report consists of a 2 x 2 table whose first column and first row are US identifiers, and the numbers inside the table are the total number (benefit part + main part) of redundancy elements between two USs; secondly, the redundant clauses related to the redundant US-pair are listed; thirdly, the text of US whose redundant phrases are marked with a hash symbol; finally, the part of the clauses in which redundant elements occur is displayed.	
\end{example}
For further evaluation purposes and easy export of the report to another platform such as Excel, a JSON report is created that collects the information about redundant US-pairs separately in a JSON object with the following entries:
\begin{itemize}
	\item Potential Redundant User Stories:  which has stored the US-pair identifier(e.g. "user\_story\_12\_AND\_user\_story\_39"). 
	
	\item Status: consisting of "Main/Beneift Part Redundancy Clauses" and "Total Redundancy Clauses", which store the count of redundancy clauses in the main and benefit part as well as in the total part of the US.
	
	\item Entity: which can consist of a "Secondary/Primary Entity" and stores the founded redundant entities.
	
	\item Common Targets/Contains: which consists of the "Main Part" and "Benefit Part" entries and only stores the targets/contains relationships that are common between the USs in a particular part of the USs. For example, if there are common redundant targets in the main part of the USs, these are included in the "Main Part" entity of the "Common Targets".
	
	\item Text: consisting of two entries, namely "First UserStory" and "Second UserStory", in which the text of the US-pair whose hash symbol has already been applied in redundant phrases is stored.
	
	\item Project Number: stores the number of the Project(e.g. "G03").
	
	\item Part of Sentence: consists of the entries "First UserStory" and "Second UserStory", in which the part of the US sentences containing redundant clauses is stored.
	
	\item All Targets/Contains: which consists of the "Main Part" and "Benefit Part" entries and stores the whole targets/contains relationships that are occurred in the particular part of the USs.
	
\end{itemize}
\begin{example}
	Listing \ref{list:json_report_sample} illustrates the example of the JSON report regarding user\_story\_12 and user\_story\_39.
\end{example}
\begin{MyListing}
	%\paragraph{}
	
	\centering
	%\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/TextualReportSample.txt}
	\includegraphics[scale=0.7]{Listing/JSONReportSample.png}
	\caption{Example of generated JSON report for one US-pair}\label{list:json_report_sample}
	
\end{MyListing}	
\subsubsection*{Step 5: Report Evaluation}
The Evaluation class, part of the \textit{org.henshin.backlog.code.evaluation} package, was developed to determine the level of redundancy in USs based on JSON reports. This class provides methods to evaluate whether two USs are either fully or partially redundant, analysing different application components of these USs.

The evaluation process involves a complex logic to determine whether USs are redundant. This includes:
\begin{itemize}
	\item Checking whether the arrays are empty or contain similar elements.
	\item Comparing the individual elements in the arrays for both USs to determine if they fully match (full redundancy) or if they have some common elements (partial redundancy).
\end{itemize}
\begin{example}
	For the two US-pairs of dataset G03, we apply the evaluation class to determine whether there is redundancy in the main or benefit part, and if so, what type of redundancy is recognised(full or partially).\\\\
	As shown in Listing \ref{list:json_evaluation}, four entries are added to the JSON report, namely "Main Partially Redundant", "Benefit Part Fully Redundant",
	"Main Part Fully Redundant", "Benefit Partially Redundant" as "Status" which their value is whether true or false. 
	\begin{MyListing}
		\paragraph{}
		
		\centering
		%\lstinputlisting[basicstyle=\ttfamily\footnotesize]{Listing/TextualReportSample.txt}
		\includegraphics[scale=0.7]{Listing/json_evaluation.png}
		\caption{Example of generated entries in JSON report regarding evaluation of level of redundancy in main or benefit part}\label{list:json_evaluation}
		
	\end{MyListing}	
	
	In the case of user\_story\_12 and user\_story\_39, the entry "Benefit Partially Redundant" was marked as \textit{true}, which means that US-pair in benefit parts are partially redundant.
	
\end{example}
The class performs these checks by iterating through the JSON arrays of Triggers, Targets and Contains and comparing each element with those in the common sections to determine redundancy.


\input{Section/Redundancy_Implementation}