\section{Analysing Redundancy}\label{redundancy}
% some explenation here about what will be explain here an why
%(Analyse von Redundanz als title und dann im Text Schreiben, dass es ein syntaktische Analyse ist
USs are a fundamental component of agile software development. They provide concise and clear descriptions of the software functionality from the end user's perspective. These stories guide the development process and ensure that the software meets the actual needs of the users. 

However, as the project backlog grows, they often become overloaded with redundant USs that can obscure project priorities, waste resources and complicate maintenance. Analysing the redundancy within these USs is key to maintaining clarity and efficiency in the development process.

The main goal of this analysis is to streamline the software development workflow by identifying redundancies in the user USs within a project's backlog.

 By identifying and consolidating overlapping functionalities, companies can avoid redundant work, streamline development work and optimise testing processes. This strategic approach not only shortens project timelines but also improves the clarity and coherence of project documentation, making it easier for team members to navigate and manage project requirements.

In section \ref{redundancy_requirement} we present the requirements and functional needs that are used as input for the design phase to fulfil the requirements. In section \ref{desing} we explain the design decisions of the workflow shown in Figure \ref{fig:design_phases} and explain how the architecture is structured. 

In section \ref{redundancy_implementation} follows the implementation to show how the components and their classes look in relation to figure \ref{fig:design_phases}. In Section \ref{redundancy_test} we show what and woh far we tested it. Finally, we apply our approach to 19 datasets of backlogs annotated with the CRF tool \footnote {https://github.com/ace-design/nlp-stories}. In Section \ref{redundancy_evaluation} we evaluate the results.

\subsection{Requirements}\label{redundancy_requirement}
In order to accomplish the analysis of redundancy in USs we try to address following requirements:
\begin{itemize}

\item "As a user, I want to perform syntactic analysis on user stories within a specified project backlog, so that I can identify and address redundancies effectively."

\item "As a User, I want a report of user story pairs that contain identical syntactic clauses in both the action and benefit parts, so that I can modify them as needed."

\item "As a User, I want to apply a filter to the redundancy report to exclusively display US-pairs, which have at least one clauses as \textit{Targets} with the terms "Action" (as a verb) and "Entity" (as a noun), so that I can efficiently reduce the number of potential redundant pairs."

\item As a user, I want to mark found redundancy clauses as Triggers with a hash symbol (\#) and show those that have a redundancy in \enquote{Persona} (as a noun) and \enquote{Action} (as a verb), so that I can better see if the persona in is also recognised as a redundancy.

\item As a user, I want to mark founded redundancy clauses as \textit{Contains} with a hash symbol (\#) and show those that contain two \enquote{Entity}(as a noun), so that I can better see whether the contained entity is also recognised as a redundancy.

\item As a user, I would like to have a redundancy report that shows founded US texts in US-pairs and adds a hash symbol (\#) at the beginning and end of the founded words as a marker, so that I can better see which words are redundant in US-pairs.

\item As a user, I want to see how many redundancy clauses were founded in main and benefit parts of each US-pair, so that I can aggregate each founded redundancy US-pair for further statistical purposes on that basis.

\item As a user, I want a table at the top of the redundancy report that lists the US-pairs and the number of redundancy clauses contained in each pair, so that I can quickly see all the US-pairs founded and the number of redundancy clauses.

\item As a user, I want to know whether the redundancy clauses belong to the main or benefit part of the US, so that I can use it for further statistical purposes.

\item As a user, I want to know if a pair of US is partially or fully redundant, so that I can delete or modify them.
\end{itemize}
% here should explain what was the reiquirement and why

\subsection{Design}\label{desing}
% what are the design decisions of this workflow that I had at the beginning, then describe what happens step by step, what translation I made, what is the main design decision you made for this translation, what question and that is part of the design
%In Desing should be how the architecture is based on which components and what is my own component, what does it use, what does the component look like, what class is there, is the class design, so everything comes from the Desing chapter
%In this section, the workflow process shown in Figure \ref{fig:design_phases} is described step by step. It also explains which design decisions were made for this workflow and how the architecture is based on which components.

%To fulfil the requirements mentioned in \ref{redundancy_requirement}, we use the CRF tool \cite{mosser2022modelling} for the annotation of backlogs and the Henshin API \cite{arendt2010henshin} under Java programming language for the automatic generation of rules for each US in the backlog. Moreover, we use Henshin's CDA(conflict and dependency analysis) feature \cite{mens2007analysing} to automatically recognise redundancy US-pairs.\\\\
This section describes the operational flow and architectural considerations that underpin the framework.

\subsubsection*{Design Overview}
%To meet the redundancy detection requirements specified in \ref{redundancy_requirement}, the system uses backlogs \cite{mosser2022modelling} annotated with CRF as the first artefact in workflow is the main input in our system. At the same time, the Henshin API \cite{arendt2010henshin}, which is used in a Java environment, enables the automatic generation of transformation rules for each US in the backlog. The use of Henshin's conflict and dependency analysis tool (CDA) \cite{mens2007analysing} is central to the automated identification of potentially redundant US-pairs, which is a cornerstone of framework design.
To address the redundancy detection requirements specified in \ref{redundancy_requirement}, our system initiates with backlogs annotated with Conditional Random Fields (CRF) as the primary input. These annotated USs are used to generate graph transformation rules for each US in the backlog using Henshin tool.

Subsequently, these rules serve as inputs for the Henshin Conflict and Dependency Analysis tool (CDA) \cite{mens2007analysing}, which automates the identification of potentially redundant US pairs. The output from the CDA tool is then utilized to create a report that compiles information on these potentially redundant pairs. This report, in turn, becomes the input for the evaluation process, which outputs statistical data concerning redundancy among USs.

Figure \ref{fig:operational_flow} illustrates how each step in this sequence is interconnected, with the output of one step feeding directly into the next. This diagram effectively demonstrates the toolchain and process workflow, highlighting how each tool transforms artefacts and contributes to the overall objective of redundancy detection.
\begin{figure}[h]
	\centering 
	\includegraphics[scale=0.4]{operational_flow}
	\caption{Step-by-step visualisation of the tool chain and its inputs and outputs}\label{fig:operational_flow}
\end{figure}

%The system ingests a graph-based model generated with the CRF tool containing a refined and annotated backlog dataset that is critical for the recognition of \textit{entities}, \textit{actions}, \textit{personas} and \textit{benefits} within the USs \cite{mosser2022modelling}. The association of each \textit{persona} with a \textit{primary action} via \textit{triggers} relationships and the association of \textit{primary/secondary actions} with \textit{primary/secondary entities} as \textit{targets} relationships, as explained by Mooser et al., form the basis for the interpretive system\cite{arulmohan2023extracting}. The output, an annotated backlog dataset as a structured JSON file, separates the annotated USs by backlog to facilitate further processing.


%As shown in Figure \ref{fig:operational_flow}, a thorough step-by-step representation of the workflow performed with different tools is provided, showing that each step has an input and an output and the output of the first step is used as input for the next step, creating a tool chain.
%along with a rationale for key design decisions and an explanation.
\paragraph{US labelling in JSON files}\label{workflow_nummerize_us}
As the annotated USs in the JSON files do not contain identifiers, a customised Python script, \textit{nummerise\_us.py}, is used to assign an unique identifier to each US, which is stored in a JSON object named \textit{"US\_Nr"} \footnote{https://github.com/amirrabieyannejad/USs\_Annotation/tree/main/Script/numberise\_us}. This addition improves the system's ability to distinguish and process individual USs within the analytical pipeline.

\paragraph{Creation of an Ecore meta model}\label{workflow_ecore}
The creation of an Ecore meta-model is essential for rule generation in Henshin. The constructed meta-model, as shown in Figure \ref{fig:ecore_meta_model}, is based on the conceptual meta-model outlined in Figure \ref{fig:conceptual_metamodel} and ensures consistency with the JSON-structured data. This correspondence between the meta-model and the JSON representation underpins the rule generation and transformation processes of the system.

\paragraph{Rule creation process}\label{workflow_rule_creator}
The transition from identified USs to implementable transformation rules within Henshin is facilitated by the package \textit{org.eclipse.emf.henshin.model. compact}. The class \textit{RuleCreator} within the package \textit{org.henshin.backlog.code.rule} is important in this process as it uses different classes to instantiate transformation rules, nodes, edges and attributes. These components are annotated with actions such as \textit{Delete}, \textit{Create} or \textit{Preserve}, which are necessary  for the subsequent application of CDA to recognise redundant US-pairs. This methodological approach improves the system's ability to detect redundancies and thus optimises the efficiency of the residue analysis process.

\paragraph{Conflict and Dependency Analysis (CDA)} After the creation of transformation rules and the creation of the corresponding Henshin file by the \textit{RuleCreator} class, the system is able to perform a conflict and dependency analysis (CDA) to identify potential redundancy pairs.

\paragraph{Extraction of text reports} The creation of a text report is a crucial step in consolidating the results of the conflict and dependency analysis (CDA), which aims to highlight essential information such as the identification of potentially redundant US-pairs, the enumeration of redundancy clauses, the categorisation of these clauses within the main or benefit parts of the USs and a tabulation of potentially redundant pairs alongside the total number of redundancy clauses.\\\\

\paragraph{Evaluating the reports} After creating the reports, we are now able to evaluate the level of redundancy between US-pairs, aiming to determine the level of redundancy in user stories (USs) based on JSON reports. 

The evaluation process involves a detailed examination of elements such as triggers, targets, and contains within the USs, comparing them to identify exact matches or significant overlaps that could indicate redundancy.

\subsubsection*{Software Architecture}\label{architectur}
In this section, we present the basic structures of our workflow and the discipline of creating such structures. Each structure comprises software elements, relations among them, and properties of both.
\begin{itemize}
	\item Annotated USs with CRF Tool: Mosser et al. integrated a dedicated Natural Language Processing (NLP) techniqueâ€”Conditional Random Fields (CRF). CRFs represent a specialized class of Markov Random Fields, tailored for discriminative modelling in pattern recognition tasks where context plays a pivotal role\cite{arulmohan2023extracting}.
	As artefact we receive a graph-based model with JSON format, which represents the refined and annotated dataset for the recognition of \emph{entities}, \emph{actions}, \emph{persons} and \emph{benefits} of USs \cite{mosser2022modelling}.
	
	%Mooser et al. have linked each \emph{Persona} to each \emph{Primary Action} as \emph{Trigger} relationships, each \emph{Primary Actions} to each \emph{Primary Entity} as \emph{Target} relationships and each \emph{Primary/Secondary Entity} to each \emph{Primary/Secondary Entity} implying a \emph{Contains} relationship\cite{arulmohan2023extracting}.
	
	\item Eclipse as IDE\footnote{https://eclipseide.org/}: Eclipse is an integrated development environment (IDE) used in computer programming. It contains a base work workspace and an extensible plug-in system for customizing the environment.
	We chose this IDE because it offers the Henshin tool specifically for model-based development.
	
	\item Eclipse Modelling Project\footnote{https://eclipse.dev/modeling/}: It focuses on the evolution and promotion of model-based development technologies within the Eclipse community by providing a unified set of modelling frameworks, tooling, and standards implementations.
	
	\item Eclipse Modelling Framework (EMF)\footnote{https://eclipse.dev/modeling/emf/}: The EMF project is a modeling framework and code generation facility for building tools and other applications based on a structured data model. From a model specification described in XMI, EMF provides tools and runtime support to produce a set of Java classes for the model, along with a set of adapter classes that enable viewing and command-based editing of the model, and a basic editor.
	
	\item Henshin\footnote{https://wiki.eclipse.org/Henshin}: Henshin is an in-place model transformation language for the Eclipse Modelling Framework (EMF). It supports direct transformations of EMF model instances (endogenous transformations), as well as generating instances of a target language from given instances of a source language (exogenous transformations).
	
	Because Henshin enables the specification of restrictions and conditions within rules, we use it to enforce and verify US requirements to ensure that the restrictions are met.
		
	\item Henshin API: It provides the specification and execution of transformation modules, units and rules. This API is beneficial due to the dynamic creation and modification of transformation modules as part of an automated tool chain\footnote{https://wiki.eclipse.org/Henshin/Compact\_API}.
	
	\item RuleCreator Class: This class developed within the \textit{org.henshin.backlog.code.rule} package, serves as an integral component of our software architecture, leveraging the Henshin API to automate the process of transformation rule creation based on dynamic JSON input. This class facilitates the programmable generation of graph transformation rules. It operates by reading JSON files, which specify USs and their associated actions, entities, and relationships, then systematically constructs corresponding Henshin modules, rules, nodes, and their attributes.
	
	the RuleCreator class effectively bridges the gap between high-level requirements specified in JSON and the low-level execution capabilities of the Henshin, ensuring that USs translated into executable transformation rules that reflect the specified software behaviour. This translation process is crucial for maintaining consistency and traceability across the software development lifecycle, from requirements specification to implementation.
	
	\item Henshin's CDA feature\footnote{https://wiki.eclipse.org/Henshin/Conflict\_and\_Dependency\_Analysis}: Henshin's conflict and dependency analysis (CDA) feature enables the detection of potential conflicts and dependencies of a set of rules.
	
	After the rules and the corresponding henshin file have been created by the RuleCreator class as artefact, we are now able to pass them to the conflict and dependency analysis (CDA) to find potential redundancy pairs.
	
	Since the analysis of conflicts and dependencies related to the \textit{attribute} is not yet considered in the CDA API, we decided to use the user interface (UI) of the CDA extension of Henshin, which supports analysis of conflict and dependencies of rules through the interactive use of CDA.
	
	\item ReportExtractor: The class from the \textit{org.henshin.backlog.code.report} package is used in our software architecture to extract and format reports from the CDA-generated \textit{Minimal-Model.ECore} that contain all information about redundant US-pairs such as redundant elements with their name and type. It uses classes from the \textit{org.eclipse.emf.ecore} package to handle EMF (Eclipse Modelling Framework) resources that support the management and manipulation of \textit{Minimal-Model.ECore} data in a structured format.
	
	In operation, the ReportExtractor class reads minimal-model.ecore files containing detailed information about redundant US-pairs. It then processes this data to generate reports in both text and JSON formats, aiding in the systematic analysis of potential redundancies within the USs. This is facilitated through methods that dynamically read and interpret the JSON data, extracting key information such as actions, entities, and their interactions.
	
	\item Evaluation class: The class is part of the \textit{org.henshin.backlog.code.evaluation} package, serves a critical function within our software architecture, focusing on the analysis and determination of redundancy between USs based on specified criteria. This class utilizes JSON processing to assess the overlap and redundancy between elements of USs, such as triggers, targets, and contains, which are vital for identifying potential redundancies in the USs.
	
	Key functionalities of this class include reading and interpreting JSON data, where it extracts detailed elements related to USs and evaluates them against redundancy criteria defined in its methods. The class method \textit{evaluateRedundancyCriteria} is particularly central, as it processes the JSON objects representing USs to determine if they are fully or partially redundant based on the presence and matching of various JSON array elements like triggers, targets, and contains in the main and benefit parts of the USs.
	
	\end{itemize}
Figure \ref{fig:technical_implementation} shows the architectural composition, highlighting the integral components and their user interface and artefacts.
\begin{figure}[h]
	\centering 
	\includegraphics[scale=0.4]{technical_implementation}
	\caption{Design phases}\label{fig:technical_implementation}
\end{figure}

%\subsubsection*{CRF Tool}\label{workflow_crf}
%As input, we receive a graph-based model generated by the CRF tool, which represents the refined and annotated dataset for the recognition of \emph{entities}, \emph{actions}, \emph{persons} and \emph{benefits} of USs \cite{mosser2022modelling}.

%Mooser et al. have linked each \emph{Persona} to each \emph{Primary Action} as \emph{Trigger} relationships, each \emph{Primary Actions} to each \emph{Primary Entity} as \emph{Target} relationships and each \emph{Primary/Secondary Entity} to each \emph{Primary/Secondary Entity} implying a \emph{Contains} relationship\cite{arulmohan2023extracting}. As output we receive a JSON-file which contains all annotated USs separated by each backlogs.
%and its level as \textit{full} or \textit{partial} redundancy in terms of clauses
Regarding redundancy, some definitions are clarified:
\begin{definition}[\textbf{Main and Benefit Parts in User Story}]
	A user story (US) is divided into two distinct parts that collectively describe what the user wants, why they want it, and how it will benefit them. These are:
	\begin{itemize}
		\item The \textit{main part}, which is essential as it clearly and concisely summarises the persona, the intended functionality and the resources required to perform the action. This part usually has follow the format: \textit{"As a [persona], I want [what]"}. 
		
		Here, the persona helps contextualise the requirement by linking it to a user type, which promotes understanding and empathy. The intended functionality describes the action that the persona wants to perform or the function they need, providing a clear statement of the requirement.
		
		Specifying the resources required to perform the action helps with planning and resource allocation and ensures that the development team is aware of the tools, technologies and time required.
		
		\item The \textit{benefit part}, which formulates the potential benefit for the end user and typically begins with the phrase \enquote{so that}. This part of the user story is essential for justifying the need for a feature by explaining the value or improvement it brings to the user's experience. It connects the functionality directly to user satisfaction, efficiency, or productivity gains, making it easier for the development team to prioritize features based on their impact.
		
		It is worth noting that sometimes the benefit part may not exist in the structure of USs. In such cases, the main part can stand alone.
	\end{itemize}
\end{definition}
\begin{definition}[\textbf{Redundancy}]
	Redundancy refers to situations where some phrases of user stories (USs) duplicate or largely overlap with others. This is the case when only a part (main or benefit part) or entire USs are syntactically identical, meaning that some or all phrases and their orderings match perfectly between two USs. \\\\
	To comprehensively assess redundancy, it is important to consider not only the textual content, but also the functional and contextual relevance of each phrase within the USs. By analysing triggers, targets and contains relationships, we can uncover redundancies that may not be immediately apparent through a simple text comparison.
\end{definition}	

\begin{definition}[\textbf{Clause}]
	In this context, a clause is constructed as a pair of words, each pair being linked by a set of predefined relational constructs. These constructs, which are central to the contextual structuring of USs, are described below:
	\begin{itemize}
		
		\item Triggers: A relationship between a persona and an action that includes the invocation of the action by the persona within the narrative of a US.
		
		\item Targets: A relationship between an action and an entity means that the action influences the entity in question.
		
		\item Contains: relation between two entities, indicating that one entity contains another one, in which one entity merges into or is enclosed by another. 
		
	\end{itemize}
	Including the specified clauses in the analysis framework ensures that the words identified in potentially redundant US-pairs are linked by one of the relational constructs outlined: \textit{Triggers}, \textit{Targets}, or \textit{Contains}. 
	
	This methodological approach supports the identification of redundancy not only on the basis of lexical similarity, but through the contextual relationships that determine the interactions within the US narratives.
\end{definition}
\begin{example}
	When evaluating the textual similarities between two USs, consider the following pair: \\
	user\_story\_01: \textit{"As a Staff member, I want to Manage Ordinances, so that I can maintain accurate ordinance information in the System."}\\
	user\_story\_02: \textit{"As a Staff member, I want to Manage Affidavits, so that I can ensure compliance with the requirements prior to the hearing."} \\\\
	Upon comparing these two user stories, we identify shared phrases: "Staff member" and "Manage." These common elements indicate that both user stories involve the same persona ("Staff member") engaged in a similar type of action ("Manage"). However, the focus of their management tasks differs significantly, with one managing "Ordinances" and the other "Affidavits."\\\\
	This distinction in the resources being managed suggests that while there are textual overlaps and commonalities in persona and action, the user stories are not duplicates, as they address different aspects of the staff member's responsibilities and goals within the organization.
\end{example}
\begin{definition}[\textbf{Full Redundancy}]
	A US-pair was categorised as "full redundancy" in the main/benefit part if all occurring clauses in the main/benefit part consisting of triggers (for the main part only), targets and contains were syntactically identical.\\
	This means that for a pair to fall into this category, every element including the wording, order and structure of the clauses relating to the triggers and targets must be identical. The assessment also extends to the "contains" elements, if these are present, to ensure that these also match perfectly without any deviation.\\
\end{definition}
This level of redundancy in main part means that the redundant story provides no additional information and can be consolidated or eliminated without compromising the completeness or operational integrity of the system specifications.

On the other hand, having this level of redundancy in the benefit part means that USs achieve the same goal; they can be categorised in the same group. This means that if the end results or benefits described by the USs are identical, regardless of the different actions/entities or triggers that lead to these results, the USs effectively serve the same purpose within the system.
\begin{example}
	For example, the following US-pair is identified as \enquote{Full Redundancy} in the main part:
	
	\textit{user\_story\_01:} \enquote{\#g14\# as a \#publisher\#, i want to \#publish\# a \#dataset\#, so that i can view just the dataset with a few people.}
	
	\textit{user\_story\_02:} \enquote{\#g14\# as a \#publisher\#, i want to \#publish\# a \#dataset\#, so that i can share the dataset publicly with everyone.}\\\\
	Since triggers (persona: "Publisher" and action: "Publish") and targets (action: "Publish" and entity: "Dataset") are identical in the main parts of the USs, we therefore have \enquote{Full Redundancy} in the \enquote{Main} parts.
\end{example}
\begin{example}
	Following US-pair is identified as \enquote{Full Redundancy} in \enquote{Benefit} part:
	
	\textit{user\_story\_02:} \#g05\# as a data publishing user, I want to be able to edit the model of data I have already imported, so that I can \#fix\# \#bugs\# or \#make\# \#enhancements\# in the \#API\# built for my \#data\#.
	
	\textit{user\_story\_07:} \#g05\# as a data publishing user, I want to be able to edit the data source of data I have already imported, so that i can \#fix\# \#bugs\# or \#make\# \#enhancements\# in the \#API\# built for my \#data\#.
	
	As we can see, targets(["fix", "bugs"], ["make", "enhancements"]) and contains (["API","enhancements"],["data","API"]) in the benefit part are identical between USs, therefore, so we have \enquote{Full Redundancy} in the \enquote{Benefit} parts.
\end{example}
\begin{definition}[\textbf{Partial Redundancy}]
	Partial redundancy in USs occurs when only certain clauses, such as targets, have significant overlap but are not completely identical. This means that while there are shared aspects such as targets between USs, other clauses such as triggers or contains may not overlap, indicating an incomplete match. Such a scenario indicates that there is substantial, but not fully, a match between the USs.
	%	Was found when only some clauses were redundant in either the Main or Benefit parts, while others were unique. This indicates overlapping clauses, but not full redundancy.
\end{definition}
\begin{example}
	Following US-pair is identified as \enquote{Partial Redundancy} in main part:
	
	\textit{user\_story\_09:} \#g04\# as a \#user\#, I want to be able to \#view\# a \#map display\# of the public recycling bins around my \#area\#.
	
	\textit{user\_story\_10:} \#g04\# as a \#user\#, I want to be able to \#view\# a \#map display\# of the special waste drop off sites around my \#area\#.
	
	As we can see, there are some redundancy clauses such as Triggers (["user", "view"]), Targets (["view", "map display"]) and Contains(["map display", "area"]) between the USs. There are also clauses such as Contains (["public recycling bins"] vs. ["hazardous waste collection points"]) that are distinct elements justifying the maintaining of separate USs. We therefore assess this as "Partial redundancy" in the "Main part".
\end{example}
\begin{example}
	Following US-pair is identified as \enquote{Partial Redundancy} in benefit part:
	
	\textit{user\_story\_17:} \#g03\# as a staff member, I want to manage approved proffers, so that I can \#ensure\# \#compliance\# with and satisfaction of the proffer in the future.
	
	\textit{user\_story\_30:} \#g03\# as a staff member, I want to manage affidavits, so that I can \#ensure\# \#compliance\# with the requirements prior to the hearing.
	
	As we can see, there is a redundancy clause as targets (["ensure", "compliance"]) between the USs in the benefit part, but there is also a clause as contains (["proffer", "satisfaction"] vs ["requirements", "hearing"]), which leads us to evaluate this as \enquote{Partial Redundancy} in the \enquote{Benefit} part.
\end{example}
\subsubsection*{Structure of a US}
The delineation and checking of redundancy clauses within USs requires a methodical approach, especially when distinguishing between the main and the benefit part of a US-pair. This distinction is crucial to ensure that redundancy identifications within one part are not mistakenly transferred to the other. Consequently, the analytical framework comprises three conditions, each of which specifies its own methodology for case processing:
\begin{itemize}
	\item Presence of benefit in both USs of the redundancy-pair: if a benefit is identifiable in each US of the pair, a process of separation is used to split the main content from the benefit parts. After this separation, a targeted search for redundancy clauses is carried out only within the main part, whereby identified redundancies are annotated with a hash symbol (\#). This process is repeated for the benefit parts to ensure a thorough check and marking of redundancies within each individual part.
	
	\item Exclusive presence of a benefit in one US of the redundancy-pair: In scenarios where only one US of the pair contains a benefit part, the analysis is limited to the main part of both USs. The aim remains the identification and annotation of redundancy clauses within this part. The lone benefit part remains in its original state and is excluded from the redundancy check.
	
	\item Absence of benefit parts in both USs of the pair: If neither of the two USs of the redundancy-pair contains a benefit part, the focus shifts completely to the main parts. The investigation is designed to highlight redundancy clauses within these parts, whereby the benefit parts are not taken into account due to their non-existence.
	
\end{itemize}
This structured and segmented approach ensures precise and efficient identification of redundancy clauses within the USs, optimising the clarity and effectiveness of textual report.




\input{Section/Redundancy_Implementation}